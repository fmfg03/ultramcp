"""
Enhanced SAM Agent with LangGraph + Graphiti + MCP Enterprise Integration

This module implements the world's first comprehensive integration of:
- MCP Protocol (Model Context Protocol) for tool integration
- LangGraph for advanced agent orchestration and state management
- Graphiti for temporal knowledge graphs and relationship awareness
- Enterprise security and monitoring

Architecture:
ğŸ§  IMMEDIATE: LangGraph state management (current conversation)
ğŸ§  SHORT-TERM: MCP Memory Analyzer (recent interactions, embeddings)  
ğŸ§  LONG-TERM: Graphiti Knowledge Graph (relationships, temporal context)
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Annotated
from dataclasses import dataclass, asdict
from enum import Enum
import uuid

# LangGraph and LangChain imports
from langgraph.graph import StateGraph, START, END, add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.base import BaseCheckpointSaver
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from typing_extensions import TypedDict

# MCP and Graphiti imports
from .enhancedMemoryService import EnhancedMemorySystem, get_enhanced_memory_system
from .memoryService import SAMMemoryAnalyzer
from ..utils.logger import logger

# LangWatch integration
try:
    import langwatch
    from langwatch.types import ChatMessage, LLMSpan
    LANGWATCH_AVAILABLE = True
    logger.info("âœ… LangWatch integration available")
except ImportError:
    LANGWATCH_AVAILABLE = False
    logger.warning("âš ï¸ LangWatch not available - install with: pip install langwatch")

# Import Graphiti
try:
    from graphiti import Graphiti
    GRAPHITI_AVAILABLE = True
except ImportError:
    logger.warning("Graphiti not available - using mock implementation")
    GRAPHITI_AVAILABLE = False

class AgentMode(Enum):
    """Different modes of operation for the enhanced agent"""
    CHAT = "chat"
    ANALYSIS = "analysis"
    TASK_EXECUTION = "task_execution"
    COLLABORATION = "collaboration"
    MEMORY_SEARCH = "memory_search"

class ContextSource(Enum):
    """Sources of context for the agent"""
    GRAPHITI_FACTS = "graphiti_facts"
    GRAPHITI_RELATIONSHIPS = "graphiti_relationships"
    GRAPHITI_TEMPORAL = "graphiti_temporal"
    MCP_MEMORIES = "mcp_memories"
    MCP_TOOLS = "mcp_tools"
    LANGGRAPH_STATE = "langgraph_state"

@dataclass
class UserContext:
    """Comprehensive user context from multiple sources"""
    user_id: str
    facts: List[str] = None
    relationships: List[Dict] = None
    recent_context: List[Dict] = None
    mcp_memories: List[Dict] = None
    interaction_patterns: Dict = None
    preferences: Dict = None
    collaboration_history: List[Dict] = None

@dataclass
class AgentDecision:
    """Represents an agent's decision in the workflow"""
    action: str
    reasoning: str
    confidence: float
    next_steps: List[str]
    context_used: List[ContextSource]
    tools_required: List[str] = None

class MCPAgentState(TypedDict):
    """Enhanced state for MCP agents with LangGraph integration"""
    messages: Annotated[List[BaseMessage], add_messages]
    user_id: str
    task_context: Dict[str, Any]
    memory_context: List[Dict]
    agent_memories: List[Dict]
    current_mode: str
    tools_used: List[str]
    collaboration_data: Dict[str, Any]
    decision_history: List[Dict]
    performance_metrics: Dict[str, Any]

class EnhancedSAMAgent:
    """
    Enhanced SAM Agent with LangGraph orchestration, Graphiti knowledge graph,
    and comprehensive MCP enterprise integration
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """Initialize the enhanced SAM agent"""
        self.config = config or {}
        self.agent_id = f"enhanced_sam_{uuid.uuid4().hex[:8]}"
        
        # Initialize core components
        self._initialize_components()
        
        # Setup LangGraph orchestration
        self._setup_langgraph()
        
        # Performance and monitoring
        self.metrics = {
            "conversations": 0,
            "successful_decisions": 0,
            "tools_executed": 0,
            "knowledge_graph_updates": 0,
            "collaboration_events": 0,
            "avg_response_time": 0.0
        }
        
        # LangWatch integration
        if LANGWATCH_AVAILABLE and self.config.get('LANGWATCH_API_KEY'):
            try:
                langwatch.login(api_key=self.config.get('LANGWATCH_API_KEY'))
                self.langwatch_enabled = True
                logger.info("ğŸ” LangWatch initialized for Enhanced SAM Agent")
            except Exception as error:
                self.langwatch_enabled = False
                logger.warning(f"âš ï¸ LangWatch initialization failed: {error}")
        else:
            self.langwatch_enabled = False
        
        logger.info(f"ğŸ¤– Enhanced SAM Agent initialized: {self.agent_id}")
    
    def _initialize_components(self):
        """Initialize all core components"""
        try:
            # MCP Memory Analyzer
            self.memory_analyzer = SAMMemoryAnalyzer()
            
            # Enhanced Memory System with Graphiti
            self.enhanced_memory = get_enhanced_memory_system(self.config)
            
            # Graphiti Knowledge Graph (if available)
            if GRAPHITI_AVAILABLE and self.config.get('OPENAI_API_KEY'):
                self.graphiti = Graphiti(
                    neo4j_uri=self.config.get('NEO4J_URI', 'bolt://localhost:7687'),
                    neo4j_user=self.config.get('NEO4J_USERNAME', 'neo4j'),
                    neo4j_password=self.config.get('NEO4J_PASSWORD', 'neo4j_password'),
                    llm_client_type="openai",
                    embedding_model=self.config.get('GRAPHITI_EMBEDDING_MODEL', 'text-embedding-3-small'),
                    llm_model=self.config.get('GRAPHITI_LLM_MODEL', 'gpt-4-turbo-preview')
                )
            else:
                self.graphiti = None
                logger.warning("Graphiti not available - using enhanced memory only")
            
            # LLM setup with multiple providers
            self.llm = self._setup_llm()
            
            # LangGraph memory saver
            self.memory_saver = MemorySaver()
            
            logger.info("âœ… Core components initialized successfully")
            
        except Exception as error:
            logger.error(f"âŒ Failed to initialize components: {error}")
            raise
    
    def _setup_llm(self):
        """Setup LLM with fallback providers"""
        providers = [
            ("openai", "OPENAI_API_KEY", ChatOpenAI),
            ("anthropic", "ANTHROPIC_API_KEY", ChatAnthropic)
        ]
        
        for provider_name, api_key_env, provider_class in providers:
            api_key = self.config.get(api_key_env)
            if api_key:
                try:
                    if provider_name == "openai":
                        return provider_class(
                            model="gpt-4-turbo-preview",
                            api_key=api_key,
                            temperature=0.1
                        )
                    elif provider_name == "anthropic":
                        return provider_class(
                            model="claude-3-sonnet-20240229",
                            api_key=api_key,
                            temperature=0.1
                        )
                except Exception as error:
                    logger.warning(f"Failed to initialize {provider_name}: {error}")
                    continue
        
        raise Exception("No LLM provider available")
    
    def _setup_langgraph(self):
        """Setup LangGraph with enhanced agent orchestration"""
        try:
            # Create state graph
            graph_builder = StateGraph(MCPAgentState)
            
            # Add nodes for different agent capabilities
            graph_builder.add_node('context_retrieval', self._context_retrieval_node)
            graph_builder.add_node('enhanced_agent', self._enhanced_agent_node)
            graph_builder.add_node('tool_execution', self._tool_execution_node)
            graph_builder.add_node('memory_storage', self._memory_storage_node)
            graph_builder.add_node('collaboration', self._collaboration_node)
            graph_builder.add_node('decision_analysis', self._decision_analysis_node)
            
            # Setup edges and routing logic
            graph_builder.add_edge(START, 'context_retrieval')
            graph_builder.add_edge('context_retrieval', 'enhanced_agent')
            
            # Conditional edges based on agent decisions
            graph_builder.add_conditional_edges(
                'enhanced_agent',
                self._route_next_step,
                {
                    'tools': 'tool_execution',
                    'collaboration': 'collaboration',
                    'memory': 'memory_storage',
                    'analysis': 'decision_analysis',
                    'end': END
                }
            )
            
            # All paths lead to memory storage
            graph_builder.add_edge('tool_execution', 'memory_storage')
            graph_builder.add_edge('collaboration', 'memory_storage')
            graph_builder.add_edge('decision_analysis', 'memory_storage')
            graph_builder.add_edge('memory_storage', END)
            
            # Compile the graph
            self.graph = graph_builder.compile(checkpointer=self.memory_saver)
            
            logger.info("ğŸ•¸ï¸ LangGraph orchestration setup complete")
            
        except Exception as error:
            logger.error(f"âŒ Failed to setup LangGraph: {error}")
            raise
    
    async def _context_retrieval_node(self, state: MCPAgentState, config: Dict = None) -> Dict:
        """Node for retrieving comprehensive context from all sources"""
        start_time = datetime.now()
        
        try:
            user_id = state.get('user_id')
            latest_message = state['messages'][-1].content if state['messages'] else ""
            
            # Get comprehensive user context
            user_context = await self._get_comprehensive_user_context(user_id, latest_message)
            
            # Update state with context
            state['memory_context'] = user_context.facts or []
            state['agent_memories'] = user_context.mcp_memories or []
            state['collaboration_data'] = {
                'relationships': user_context.relationships or [],
                'interaction_patterns': user_context.interaction_patterns or {},
                'collaboration_history': user_context.collaboration_history or []
            }
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"ğŸ“‹ Context retrieved", {
                "user_id": user_id,
                "facts_count": len(user_context.facts or []),
                "memories_count": len(user_context.mcp_memories or []),
                "relationships_count": len(user_context.relationships or []),
                "processing_time": processing_time
            })
            
            return state
            
        except Exception as error:
            logger.error(f"âŒ Context retrieval failed: {error}")
            # Continue with empty context rather than failing
            return state
    
    async def _enhanced_agent_node(self, state: MCPAgentState, config: Dict = None) -> Dict:
        """Enhanced agent node with full contextual awareness"""
        start_time = datetime.now()
        
        try:
            # Build contextual system prompt
            system_prompt = self._build_contextual_prompt(state)
            
            # Prepare messages for LLM
            messages = [system_prompt] + state['messages']
            
            # Get LLM response with full context
            response = await self.llm.ainvoke(messages)
            
            # Analyze the decision made
            decision = self._analyze_agent_decision(response.content, state)
            
            # Update state with response and decision
            state['messages'] = state['messages'] + [response]
            state['decision_history'] = state.get('decision_history', []) + [asdict(decision)]
            
            # Async store to knowledge graph (non-blocking)
            if self.graphiti:
                asyncio.create_task(self._store_interaction_to_graphiti(state, response))
            
            processing_time = (datetime.now() - start_time).total_seconds()
            self.metrics["avg_response_time"] = (
                self.metrics["avg_response_time"] + processing_time
            ) / 2
            
            logger.info(f"ğŸ§  Enhanced agent response generated", {
                "decision_action": decision.action,
                "confidence": decision.confidence,
                "processing_time": processing_time,
                "context_sources": len(decision.context_used)
            })
            
            return state
            
        except Exception as error:
            logger.error(f"âŒ Enhanced agent node failed: {error}")
            # Fallback response
            fallback_response = AIMessage(content="I apologize, but I encountered an issue processing your request. Please try again.")
            state['messages'] = state['messages'] + [fallback_response]
            return state
    
    async def _tool_execution_node(self, state: MCPAgentState, config: Dict = None) -> Dict:
        """Node for executing MCP tools based on agent decisions"""
        try:
            latest_decision = state.get('decision_history', [])[-1] if state.get('decision_history') else {}
            tools_required = latest_decision.get('tools_required', [])
            
            if not tools_required:
                return state
            
            execution_results = []
            for tool_name in tools_required:
                try:
                    # Mock tool execution - replace with actual MCP tool calls
                    result = await self._execute_mcp_tool(tool_name, state)
                    execution_results.append({
                        'tool': tool_name,
                        'result': result,
                        'success': True
                    })
                    self.metrics["tools_executed"] += 1
                    
                except Exception as tool_error:
                    execution_results.append({
                        'tool': tool_name,
                        'error': str(tool_error),
                        'success': False
                    })
            
            # Update state with tool results
            state['tools_used'] = state.get('tools_used', []) + tools_required
            state['task_context']['tool_results'] = execution_results
            
            logger.info(f"ğŸ”§ Tools executed", {
                "tools_count": len(tools_required),
                "successful": sum(1 for r in execution_results if r['success']),
                "failed": sum(1 for r in execution_results if not r['success'])
            })
            
            return state
            
        except Exception as error:
            logger.error(f"âŒ Tool execution failed: {error}")
            return state
    
    async def _memory_storage_node(self, state: MCPAgentState, config: Dict = None) -> Dict:
        """Node for storing conversation and context in memory systems"""
        try:
            user_id = state.get('user_id')
            latest_message = state['messages'][-1] if state['messages'] else None
            
            if latest_message and isinstance(latest_message, AIMessage):
                # Store in enhanced memory system
                context = {
                    'user_id': user_id,
                    'conversation_context': state.get('task_context', {}),
                    'decision_history': state.get('decision_history', []),
                    'tools_used': state.get('tools_used', []),
                    'collaboration_data': state.get('collaboration_data', {}),
                    'timestamp': datetime.now().isoformat(),
                    'agent_id': self.agent_id
                }
                
                # Store in both MCP and Graphiti systems
                await self.enhanced_memory.store_memory(
                    content=latest_message.content,
                    context=context
                )
                
                self.metrics["knowledge_graph_updates"] += 1
            
            logger.info("ğŸ’¾ Conversation stored in memory systems")
            return state
            
        except Exception as error:
            logger.error(f"âŒ Memory storage failed: {error}")
            return state
    
    async def _collaboration_node(self, state: MCPAgentState, config: Dict = None) -> Dict:
        """Node for multi-agent collaboration via shared knowledge graph"""
        try:
            collaboration_data = state.get('collaboration_data', {})
            user_id = state.get('user_id')
            
            if self.graphiti and user_id:
                # Get collaboration opportunities with other agents
                collaboration_context = await self._get_collaboration_opportunities(user_id)
                
                # Update collaboration data
                collaboration_data.update({
                    'opportunities': collaboration_context.get('opportunities', []),
                    'shared_knowledge': collaboration_context.get('shared_knowledge', {}),
                    'recommended_agents': collaboration_context.get('recommended_agents', [])
                })
                
                state['collaboration_data'] = collaboration_data
                self.metrics["collaboration_events"] += 1
            
            logger.info("ğŸ¤ Collaboration analysis completed")
            return state
            
        except Exception as error:
            logger.error(f"âŒ Collaboration node failed: {error}")
            return state
    
    async def _decision_analysis_node(self, state: MCPAgentState, config: Dict = None) -> Dict:
        """Node for analyzing and optimizing agent decisions"""
        try:
            decision_history = state.get('decision_history', [])
            
            if decision_history:
                latest_decision = decision_history[-1]
                
                # Analyze decision quality and patterns
                analysis = {
                    'decision_confidence': latest_decision.get('confidence', 0),
                    'context_utilization': len(latest_decision.get('context_used', [])),
                    'reasoning_quality': self._evaluate_reasoning_quality(latest_decision.get('reasoning', '')),
                    'prediction_accuracy': await self._calculate_prediction_accuracy(state)
                }
                
                # Update performance metrics
                state['performance_metrics'] = analysis
                
                if analysis['decision_confidence'] > 0.8:
                    self.metrics["successful_decisions"] += 1
            
            logger.info("ğŸ“Š Decision analysis completed")
            return state
            
        except Exception as error:
            logger.error(f"âŒ Decision analysis failed: {error}")
            return state
    
    def _route_next_step(self, state: MCPAgentState) -> str:
        """Determine the next step in the workflow based on agent decision"""
        try:
            decision_history = state.get('decision_history', [])
            if not decision_history:
                return 'end'
            
            latest_decision = decision_history[-1]
            action = latest_decision.get('action', '')
            
            # Route based on the action determined by the agent
            if 'tool' in action.lower() or 'execute' in action.lower():
                return 'tools'
            elif 'collaborate' in action.lower() or 'agent' in action.lower():
                return 'collaboration'
            elif 'analyze' in action.lower() or 'decision' in action.lower():
                return 'analysis'
            elif 'remember' in action.lower() or 'store' in action.lower():
                return 'memory'
            else:
                return 'end'
                
        except Exception as error:
            logger.error(f"âŒ Routing decision failed: {error}")
            return 'end'
    
    async def _get_comprehensive_user_context(self, user_id: str, query: str) -> UserContext:
        """Get comprehensive user context from all available sources"""
        context = UserContext(user_id=user_id)
        
        try:
            # Get Graphiti context if available
            if self.graphiti:
                graphiti_context = await self._get_graphiti_context(user_id, query)
                context.facts = graphiti_context.get('facts', [])
                context.relationships = graphiti_context.get('relationships', [])
                context.recent_context = graphiti_context.get('recent_context', [])
                context.interaction_patterns = graphiti_context.get('patterns', {})
                context.collaboration_history = graphiti_context.get('collaboration_history', [])
            
            # Get MCP memories
            mcp_memories = await self._get_mcp_memories(user_id, query)
            context.mcp_memories = mcp_memories
            
            # Get user preferences from context
            context.preferences = await self._get_user_preferences(user_id)
            
        except Exception as error:
            logger.error(f"âŒ Failed to get comprehensive context: {error}")
        
        return context
    
    async def _get_graphiti_context(self, user_id: str, query: str) -> Dict[str, Any]:
        """Get user context from Graphiti knowledge graph"""
        try:
            if not self.graphiti:
                return {}
            
            # Search for user-related facts
            user_facts = await self.graphiti.search(
                query=f"user:{user_id} {query}",
                search_type="hybrid",
                limit=10
            )
            
            # Get user's relationship network
            user_relationships = await self._get_user_relationships(user_id)
            
            # Get temporal context (recent interactions)
            recent_context = await self._get_recent_context(user_id, query)
            
            # Get interaction patterns
            patterns = await self._analyze_user_patterns(user_id)
            
            # Get collaboration history
            collaboration_history = await self._get_collaboration_history(user_id)
            
            return {
                "facts": user_facts.get('results', []) if isinstance(user_facts, dict) else [],
                "relationships": user_relationships,
                "recent_context": recent_context,
                "patterns": patterns,
                "collaboration_history": collaboration_history
            }
            
        except Exception as error:
            logger.error(f"âŒ Error getting Graphiti context: {error}")
            return {}
    
    async def _get_mcp_memories(self, user_id: str, query: str) -> List[Dict]:
        """Get relevant memories from MCP memory system"""
        try:
            # Use enhanced memory system for search
            search_results = await self.enhanced_memory.enhanced_search(
                query=query,
                user_context={'user_id': user_id},
                limit=5
            )
            
            return [
                {
                    'content': result.content,
                    'relevance_score': result.relevance_score,
                    'source': result.source,
                    'metadata': result.metadata
                }
                for result in search_results
            ]
            
        except Exception as error:
            logger.error(f"âŒ Error getting MCP memories: {error}")
            return []
    
    def _build_contextual_prompt(self, state: MCPAgentState) -> SystemMessage:
        """Build comprehensive system prompt with all available context"""
        user_id = state.get('user_id', 'unknown')
        memory_context = state.get('memory_context', [])
        agent_memories = state.get('agent_memories', [])
        collaboration_data = state.get('collaboration_data', {})
        task_context = state.get('task_context', {})
        
        # Build facts string from Graphiti
        facts_string = ""
        if memory_context:
            facts_string = "\n".join([f"- {fact}" for fact in memory_context[:10]])
        
        # Build memories string from MCP
        memories_string = ""
        if agent_memories:
            memories_string = "\n".join([
                f"- {memory.get('content', '')[:100]}..." 
                for memory in agent_memories[:5]
            ])
        
        # Build relationships string
        relationships_string = ""
        relationships = collaboration_data.get('relationships', [])
        if relationships:
            relationships_string = "\n".join([
                f"- {rel.get('description', '')}" 
                for rel in relationships[:5]
            ])
        
        # Build collaboration context
        collaboration_string = ""
        opportunities = collaboration_data.get('opportunities', [])
        if opportunities:
            collaboration_string = "\n".join([
                f"- {opp.get('description', '')}" 
                for opp in opportunities[:3]
            ])
        
        # Current task context
        current_task = task_context.get('type', 'general_assistance')
        task_details = task_context.get('details', '')
        
        return SystemMessage(content=f"""
You are SAM, an advanced AI agent with comprehensive contextual awareness through multiple intelligence layers:

ğŸ§  TRIPLE-LAYER MEMORY SYSTEM:
- IMMEDIATE: Current conversation state and context
- SHORT-TERM: Recent interactions and learned patterns  
- LONG-TERM: Knowledge graph relationships and temporal context

ğŸ‘¤ USER CONTEXT (ID: {user_id}):
KNOWLEDGE GRAPH FACTS:
{facts_string or 'No specific facts available'}

RELEVANT MEMORIES:
{memories_string or 'No relevant memories'}

RELATIONSHIP NETWORK:
{relationships_string or 'No relationships mapped'}

ğŸ¤ COLLABORATION OPPORTUNITIES:
{collaboration_string or 'No collaboration opportunities identified'}

ğŸ“‹ CURRENT TASK: {current_task}
{task_details}

ğŸ¯ CAPABILITIES:
- Access to comprehensive user history and preferences
- Temporal reasoning about relationship evolution
- Multi-agent collaboration through shared knowledge graphs
- Predictive assistance based on interaction patterns
- Tool execution for complex tasks

INSTRUCTIONS:
1. Use ALL available context to provide informed, personalized responses
2. Demonstrate understanding of user's history and preferences
3. Identify opportunities for tool usage or agent collaboration
4. Provide reasoning for your decisions and next steps
5. Maintain conversation continuity using temporal context

Respond with empathy, intelligence, and contextual awareness that reflects the depth of available information.
        """)
    
    def _analyze_agent_decision(self, response_content: str, state: MCPAgentState) -> AgentDecision:
        """Analyze the agent's response to extract decision information"""
        # This is a simplified analysis - in production, you might use NLP to extract decisions
        
        action = "respond"
        reasoning = "Generated contextual response based on available information"
        confidence = 0.8
        next_steps = []
        context_used = []
        tools_required = []
        
        # Analyze response content for decision indicators
        if any(keyword in response_content.lower() for keyword in ['search', 'find', 'look up']):
            action = "search_memory"
            tools_required.append('memory_search')
            
        elif any(keyword in response_content.lower() for keyword in ['calculate', 'compute', 'analyze']):
            action = "execute_analysis"
            tools_required.append('analysis_tool')
            
        elif any(keyword in response_content.lower() for keyword in ['collaborate', 'work with', 'coordinate']):
            action = "initiate_collaboration"
            next_steps.append('contact_other_agents')
            
        elif any(keyword in response_content.lower() for keyword in ['remember', 'save', 'store']):
            action = "store_information"
            next_steps.append('update_knowledge_graph')
        
        # Determine context sources used
        if state.get('memory_context'):
            context_used.append(ContextSource.GRAPHITI_FACTS)
        if state.get('agent_memories'):
            context_used.append(ContextSource.MCP_MEMORIES)
        if state.get('collaboration_data'):
            context_used.append(ContextSource.GRAPHITI_RELATIONSHIPS)
        
        # Adjust confidence based on available context
        if len(context_used) >= 3:
            confidence = min(0.95, confidence + 0.15)
        elif len(context_used) >= 2:
            confidence = min(0.9, confidence + 0.1)
        
        return AgentDecision(
            action=action,
            reasoning=reasoning,
            confidence=confidence,
            next_steps=next_steps,
            context_used=context_used,
            tools_required=tools_required
        )
    
    async def _store_interaction_to_graphiti(self, state: MCPAgentState, response: AIMessage):
        """Store interaction in Graphiti knowledge graph (async, non-blocking)"""
        try:
            if not self.graphiti:
                return
            
            user_id = state.get('user_id')
            latest_user_message = None
            
            # Find the latest user message
            for msg in reversed(state['messages']):
                if isinstance(msg, HumanMessage):
                    latest_user_message = msg
                    break
            
            if latest_user_message and user_id:
                episode = {
                    "text": f"User {user_id}: {latest_user_message.content}\nSAM Agent: {response.content}",
                    "metadata": {
                        "user_id": user_id,
                        "agent": "enhanced_sam",
                        "agent_id": self.agent_id,
                        "context": state.get('task_context', {}),
                        "decision_history": state.get('decision_history', []),
                        "tools_used": state.get('tools_used', []),
                        "timestamp": datetime.now().isoformat(),
                        "conversation_turn": len(state['messages']) // 2
                    }
                }
                
                result = await self.graphiti.add_episode(episode)
                logger.info(f"ğŸ“ Stored episode in Graphiti: {result.get('episode_id', 'unknown')}")
                
        except Exception as error:
            logger.error(f"âŒ Error storing to Graphiti: {error}")
    
    # Placeholder methods for advanced features
    
    async def _get_user_relationships(self, user_id: str) -> List[Dict]:
        """Get user relationships from knowledge graph"""
        # Placeholder - implement with actual Graphiti queries
        return []
    
    async def _get_recent_context(self, user_id: str, query: str) -> List[Dict]:
        """Get recent temporal context"""
        # Placeholder - implement with Graphiti temporal queries
        return []
    
    async def _analyze_user_patterns(self, user_id: str) -> Dict:
        """Analyze user interaction patterns"""
        # Placeholder - implement pattern analysis
        return {}
    
    async def _get_collaboration_history(self, user_id: str) -> List[Dict]:
        """Get collaboration history with other agents"""
        # Placeholder - implement collaboration tracking
        return []
    
    async def _get_user_preferences(self, user_id: str) -> Dict:
        """Get user preferences from various sources"""
        # Placeholder - implement preference extraction
        return {}
    
    async def _get_collaboration_opportunities(self, user_id: str) -> Dict:
        """Identify collaboration opportunities with other agents"""
        # Placeholder - implement collaboration analysis
        return {"opportunities": [], "shared_knowledge": {}, "recommended_agents": []}
    
    async def _execute_mcp_tool(self, tool_name: str, state: MCPAgentState) -> Dict:
        """Execute an MCP tool"""
        # Placeholder - implement actual MCP tool execution
        return {"result": f"Executed {tool_name}", "success": True}
    
    def _evaluate_reasoning_quality(self, reasoning: str) -> float:
        """Evaluate the quality of agent reasoning"""
        # Simple heuristic - implement more sophisticated evaluation
        if len(reasoning) > 50 and any(word in reasoning.lower() for word in ['because', 'since', 'therefore']):
            return 0.8
        return 0.5
    
    async def _calculate_prediction_accuracy(self, state: MCPAgentState) -> float:
        """Calculate prediction accuracy based on historical decisions"""
        # Placeholder - implement actual prediction accuracy calculation
        return 0.75
    
    # Public interface methods
    
    async def chat(
        self,
        message: str,
        user_id: str,
        task_context: Dict[str, Any] = None,
        mode: AgentMode = AgentMode.CHAT
    ) -> Dict[str, Any]:
        """
        Main chat interface for the enhanced SAM agent
        
        Args:
            message: User message
            user_id: Unique user identifier
            task_context: Additional task context
            mode: Agent operation mode
            
        Returns:
            Dict containing response and metadata
        """
        start_time = datetime.now()
        trace_id = None
        
        # Initialize LangWatch trace
        if self.langwatch_enabled:
            try:
                trace_id = f"enhanced_sam_{user_id}_{int(start_time.timestamp())}"
                langwatch.get_current_trace().trace_id = trace_id
                langwatch.get_current_trace().user_id = user_id
                langwatch.get_current_trace().metadata = {
                    "agent_type": "enhanced_sam",
                    "agent_id": self.agent_id,
                    "mode": mode.value,
                    "task_context": task_context or {},
                    "langgraph_integration": True,
                    "graphiti_integration": True
                }
                langwatch.get_current_trace().tags = [
                    "enhanced_sam",
                    "langgraph",
                    "graphiti",
                    "mcp_enterprise",
                    mode.value
                ]
            except Exception as error:
                logger.warning(f"âš ï¸ LangWatch trace initialization failed: {error}")
        
        try:
            self.metrics["conversations"] += 1
            
            # Prepare initial state
            initial_state = {
                "messages": [HumanMessage(content=message)],
                "user_id": user_id,
                "task_context": task_context or {},
                "memory_context": [],
                "agent_memories": [],
                "current_mode": mode.value,
                "tools_used": [],
                "collaboration_data": {},
                "decision_history": [],
                "performance_metrics": {}
            }
            
            # Configuration for LangGraph execution
            config = {
                "configurable": {
                    "thread_id": f"{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                }
            }
            
            # Execute the graph
            result = await self.graph.ainvoke(initial_state, config=config)
            
            # Extract response
            final_message = result['messages'][-1] if result['messages'] else None
            response_content = final_message.content if final_message else "I apologize, but I couldn't generate a response."
            
            duration = (datetime.now() - start_time).total_seconds()
            
            # Complete LangWatch trace
            if self.langwatch_enabled and trace_id:
                try:
                    langwatch.get_current_trace().update({
                        "output": response_content,
                        "metrics": {
                            "processing_time": duration,
                            "context_sources": len(result.get('memory_context', [])),
                            "tools_used": len(result.get('tools_used', [])),
                            "collaboration_events": len(result.get('collaboration_data', {}).get('opportunities', [])),
                            "decision_confidence": result.get('decision_history', [{}])[-1].get('confidence', 0) if result.get('decision_history') else 0,
                            "knowledge_graph_updates": 1 if self.graphiti else 0
                        }
                    })
                except Exception as error:
                    logger.warning(f"âš ï¸ LangWatch trace completion failed: {error}")
            
            # Return comprehensive response
            return {
                "response": response_content,
                "metadata": {
                    "agent_id": self.agent_id,
                    "user_id": user_id,
                    "processing_time": duration,
                    "mode": mode.value,
                    "context_sources": len(result.get('memory_context', [])),
                    "tools_used": result.get('tools_used', []),
                    "collaboration_data": result.get('collaboration_data', {}),
                    "decision_history": result.get('decision_history', []),
                    "performance_metrics": result.get('performance_metrics', {}),
                    "langwatch_trace_id": trace_id
                },
                "conversation_state": result
            }
            
        except Exception as error:
            logger.error(f"âŒ Chat failed: {error}")
            return {
                "response": "I apologize, but I encountered an error processing your request. Please try again.",
                "error": str(error),
                "metadata": {
                    "agent_id": self.agent_id,
                    "user_id": user_id,
                    "processing_time": (datetime.now() - start_time).total_seconds(),
                    "mode": mode.value
                }
            }
    
    async def get_metrics(self) -> Dict[str, Any]:
        """Get agent performance metrics"""
        return {
            **self.metrics,
            "agent_id": self.agent_id,
            "uptime": "calculated_uptime",  # Implement actual uptime calculation
            "memory_usage": "calculated_memory",  # Implement memory usage tracking
            "knowledge_graph_size": await self._get_knowledge_graph_size(),
            "timestamp": datetime.now().isoformat()
        }
    
    async def _get_knowledge_graph_size(self) -> Dict[str, int]:
        """Get knowledge graph statistics"""
        if self.graphiti:
            try:
                # Placeholder - implement actual graph size queries
                return {"nodes": 0, "relationships": 0, "episodes": 0}
            except Exception:
                pass
        return {"nodes": 0, "relationships": 0, "episodes": 0}
    
    async def health_check(self) -> Dict[str, Any]:
        """Comprehensive health check"""
        health = {
            "status": "healthy",
            "components": {},
            "timestamp": datetime.now().isoformat()
        }
        
        # Check core components
        try:
            health["components"]["enhanced_memory"] = "healthy"
            health["components"]["llm"] = "healthy"
            health["components"]["langgraph"] = "healthy"
            
            if self.graphiti:
                health["components"]["graphiti"] = "healthy"
            else:
                health["components"]["graphiti"] = "not_available"
                health["status"] = "degraded"
                
        except Exception as error:
            health["status"] = "unhealthy"
            health["error"] = str(error)
        
        return health
    
    async def close(self):
        """Clean up resources"""
        try:
            if self.graphiti:
                await self.graphiti.close()
            await self.enhanced_memory.close()
            logger.info(f"ğŸ”’ Enhanced SAM Agent {self.agent_id} closed successfully")
        except Exception as error:
            logger.error(f"âŒ Error closing agent: {error}")

# Global instance management
_enhanced_sam_agents = {}

def get_enhanced_sam_agent(agent_id: str = None, config: Dict[str, Any] = None) -> EnhancedSAMAgent:
    """Get or create an enhanced SAM agent instance"""
    global _enhanced_sam_agents
    
    if agent_id is None:
        agent_id = "default"
    
    if agent_id not in _enhanced_sam_agents:
        _enhanced_sam_agents[agent_id] = EnhancedSAMAgent(config)
    
    return _enhanced_sam_agents[agent_id]

async def cleanup_all_agents():
    """Clean up all agent instances"""
    global _enhanced_sam_agents
    
    for agent in _enhanced_sam_agents.values():
        await agent.close()
    
    _enhanced_sam_agents.clear()