/**
 * UltraMCP Integration Commands for Claudia
 * 
 * Rust backend commands that bridge Claudia's GUI with UltraMCP's
 * hybrid local+API multi-LLM capabilities
 */

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::process::{Command, Stdio};
use std::time::{SystemTime, UNIX_EPOCH};
use tokio::process::Command as AsyncCommand;
use tokio::io::{AsyncBufReadExt, BufReader};
use tauri::{command, State};

// =============================================================================
// DATA STRUCTURES
// =============================================================================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DebateConfig {
    pub topic: String,
    pub mode: String, // "local", "hybrid", "privacy", "cost_optimized"
    pub participants: Vec<String>,
    pub max_rounds: u32,
    pub confidence_threshold: f64,
    pub privacy_mode: bool,
    pub cost_budget: Option<f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DebateResult {
    pub id: String,
    pub topic: String,
    pub consensus: String,
    pub confidence: f64,
    pub status: String,
    pub cost_breakdown: CostBreakdown,
    pub privacy_score: f64,
    pub participants_used: Vec<String>,
    pub rounds_completed: u32,
    pub metadata: DebateMetadata,
    pub timestamp: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CostBreakdown {
    pub local: f64,
    pub api: f64,
    pub total: f64,
    pub savings: f64,
    pub savings_percentage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DebateMetadata {
    pub total_tokens: u32,
    pub avg_response_time: f64,
    pub models_used: HashMap<String, u32>,
    pub privacy_events: Vec<String>,
    pub optimization_applied: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocalModel {
    pub id: String,
    pub name: String,
    pub version: String,
    pub size: String,
    pub ram_usage: String,
    pub context_length: u32,
    pub specialization: String,
    pub role: String,
    pub status: String, // "active", "idle", "loading", "error", "stopped"
    pub performance: ModelPerformance,
    pub capabilities: Vec<String>,
    pub cost_per_token: f64,
    pub privacy_score: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelPerformance {
    pub avg_response_time: f64,
    pub tokens_per_second: f64,
    pub total_requests: u32,
    pub avg_confidence: f64,
    pub uptime: f64,
    pub last_used: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemMetrics {
    pub total_models: u32,
    pub active_models: u32,
    pub total_storage: String,
    pub total_ram_usage: String,
    pub combined_tokens_per_second: f64,
    pub total_requests: u32,
    pub avg_confidence: f64,
    pub cost_savings: f64,
    pub privacy_score: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CostAnalytics {
    pub current_costs: CostBreakdown,
    pub model_costs: Vec<ModelCost>,
    pub savings_calculation: SavingsCalculation,
    pub optimization_recommendations: Vec<OptimizationRecommendation>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelCost {
    pub model_name: String,
    pub model_type: String, // "local" or "api"
    pub requests: u32,
    pub tokens: u32,
    pub cost: f64,
    pub avg_cost_per_request: f64,
    pub percentage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SavingsCalculation {
    pub total_api_equivalent: f64,
    pub actual_cost: f64,
    pub savings: f64,
    pub savings_percentage: f64,
    pub local_request_count: u32,
    pub api_request_count: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizationRecommendation {
    pub id: String,
    pub recommendation_type: String,
    pub title: String,
    pub description: String,
    pub potential_savings: f64,
    pub impact: String,
    pub effort: String,
}

// =============================================================================
// ULTRAMCP COMMAND EXECUTOR
// =============================================================================

pub struct UltraMCPExecutor {
    pub base_path: String,
}

impl UltraMCPExecutor {
    pub fn new() -> Self {
        Self {
            base_path: "/root/ultramcp".to_string(),
        }
    }

    pub async fn execute_command(&self, command: &str, args: Vec<&str>) -> Result<String, String> {
        let output = AsyncCommand::new("make")
            .arg(command)
            .args(args)
            .current_dir(&self.base_path)
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .spawn()
            .map_err(|e| format!("Failed to spawn command: {}", e))?;

        let result = output
            .wait_with_output()
            .await
            .map_err(|e| format!("Failed to execute command: {}", e))?;

        if result.status.success() {
            Ok(String::from_utf8_lossy(&result.stdout).to_string())
        } else {
            Err(String::from_utf8_lossy(&result.stderr).to_string())
        }
    }

    pub async fn execute_streaming_command<F>(&self, command: &str, args: Vec<&str>, callback: F) -> Result<String, String>
    where
        F: Fn(String) + Send + 'static,
    {
        let mut child = AsyncCommand::new("make")
            .arg(command)
            .args(args)
            .current_dir(&self.base_path)
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .spawn()
            .map_err(|e| format!("Failed to spawn streaming command: {}", e))?;

        if let Some(stdout) = child.stdout.take() {
            let reader = BufReader::new(stdout);
            let mut lines = reader.lines();

            while let Some(line) = lines.next_line().await.unwrap_or(None) {
                callback(line);
            }
        }

        let output = child
            .wait_with_output()
            .await
            .map_err(|e| format!("Failed to complete streaming command: {}", e))?;

        if output.status.success() {
            Ok("Command completed successfully".to_string())
        } else {
            Err(String::from_utf8_lossy(&output.stderr).to_string())
        }
    }
}

// =============================================================================
// TAURI COMMANDS
// =============================================================================

#[command]
pub async fn start_cod_debate(
    config: DebateConfig,
    executor: State<'_, UltraMCPExecutor>,
) -> Result<String, String> {
    let topic_arg = format!("TOPIC={}", config.topic);
    let mode_command = format!("cod-{}", config.mode);
    
    println!("üé≠ Starting CoD debate: {} with mode: {}", config.topic, config.mode);
    
    let result = executor
        .execute_command(&mode_command, vec![&topic_arg])
        .await?;
    
    // Generate unique debate ID
    let debate_id = format!("debate_{}", 
        SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs()
    );
    
    Ok(debate_id)
}

#[command]
pub async fn get_local_models(
    executor: State<'_, UltraMCPExecutor>,
) -> Result<Vec<LocalModel>, String> {
    println!("ü§ñ Fetching local models...");
    
    let output = executor
        .execute_command("local-models", vec![])
        .await?;
    
    // Parse the output and create model objects
    let models = parse_local_models_output(&output)?;
    
    Ok(models)
}

#[command]
pub async fn get_local_model_status(
    executor: State<'_, UltraMCPExecutor>,
) -> Result<SystemMetrics, String> {
    println!("üìä Getting local model status...");
    
    let output = executor
        .execute_command("local-status", vec![])
        .await?;
    
    let metrics = parse_system_metrics(&output)?;
    
    Ok(metrics)
}

#[command]
pub async fn start_local_model(
    model_id: String,
    executor: State<'_, UltraMCPExecutor>,
) -> Result<String, String> {
    println!("‚ñ∂Ô∏è Starting local model: {}", model_id);
    
    // This would typically start a specific model
    // For now, we'll use a generic command
    let result = executor
        .execute_command("local-status", vec![])
        .await?;
    
    Ok(format!("Model {} start initiated", model_id))
}

#[command]
pub async fn stop_local_model(
    model_id: String,
    executor: State<'_, UltraMCPExecutor>,
) -> Result<String, String> {
    println!("‚èπÔ∏è Stopping local model: {}", model_id);
    
    // This would typically stop a specific model
    Ok(format!("Model {} stop initiated", model_id))
}

#[command]
pub async fn get_debate_results(
    debate_id: String,
    executor: State<'_, UltraMCPExecutor>,
) -> Result<Option<DebateResult>, String> {
    println!("üìã Fetching debate results for: {}", debate_id);
    
    // Try to find debate results in the data directories
    let result = load_debate_results(&debate_id).await?;
    
    Ok(result)
}

#[command]
pub async fn get_cost_analytics(
    time_range: String,
    executor: State<'_, UltraMCPExecutor>,
) -> Result<CostAnalytics, String> {
    println!("üí∞ Generating cost analytics for: {}", time_range);
    
    // Generate cost analytics from system data
    let analytics = generate_cost_analytics(&time_range).await?;
    
    Ok(analytics)
}

#[command]
pub async fn run_local_chat(
    message: String,
    model: Option<String>,
    executor: State<'_, UltraMCPExecutor>,
) -> Result<String, String> {
    println!("üí¨ Running local chat with message: {}", message);
    
    let text_arg = format!("TEXT={}", message);
    let result = executor
        .execute_command("local-chat", vec![&text_arg])
        .await?;
    
    Ok(result)
}

#[command]
pub async fn get_system_health(
    executor: State<'_, UltraMCPExecutor>,
) -> Result<HashMap<String, String>, String> {
    println!("üè• Checking system health...");
    
    let output = executor
        .execute_command("health-check", vec![])
        .await?;
    
    let health_status = parse_health_output(&output)?;
    
    Ok(health_status)
}

#[command]
pub async fn optimize_costs(
    optimization_type: String,
    executor: State<'_, UltraMCPExecutor>,
) -> Result<String, String> {
    println!("‚ö° Applying cost optimization: {}", optimization_type);
    
    // Apply specific optimization strategies
    match optimization_type.as_str() {
        "prefer_local" => {
            // Configure system to prefer local models
            Ok("Local model preference enabled".to_string())
        },
        "batch_requests" => {
            // Enable request batching
            Ok("Request batching enabled".to_string())
        },
        "cache_results" => {
            // Enable result caching
            Ok("Result caching enabled".to_string())
        },
        _ => Err("Unknown optimization type".to_string()),
    }
}

// =============================================================================
// HELPER FUNCTIONS
// =============================================================================

fn parse_local_models_output(output: &str) -> Result<Vec<LocalModel>, String> {
    // Parse the output from `make local-models` command
    // This is a simplified parser - in reality, you'd parse the actual output format
    
    let sample_models = vec![
        LocalModel {
            id: "qwen-25-14b".to_string(),
            name: "Qwen 2.5 14B".to_string(),
            version: "14b".to_string(),
            size: "9.0 GB".to_string(),
            ram_usage: "~12 GB".to_string(),
            context_length: 32000,
            specialization: "Complex reasoning, strategic analysis".to_string(),
            role: "Strategic Analyst".to_string(),
            status: "active".to_string(),
            performance: ModelPerformance {
                avg_response_time: 32.5,
                tokens_per_second: 15.2,
                total_requests: 147,
                avg_confidence: 0.92,
                uptime: 98.5,
                last_used: "2024-01-15T10:30:00Z".to_string(),
            },
            capabilities: vec![
                "Strategic Planning".to_string(),
                "Complex Analysis".to_string(),
                "Research".to_string(),
                "Decision Making".to_string(),
            ],
            cost_per_token: 0.0,
            privacy_score: 100.0,
        },
        LocalModel {
            id: "llama-31-8b".to_string(),
            name: "Llama 3.1 8B".to_string(),
            version: "8b".to_string(),
            size: "4.9 GB".to_string(),
            ram_usage: "~7 GB".to_string(),
            context_length: 128000,
            specialization: "High-quality general analysis".to_string(),
            role: "Balanced Reasoner".to_string(),
            status: "active".to_string(),
            performance: ModelPerformance {
                avg_response_time: 17.5,
                tokens_per_second: 28.4,
                total_requests: 203,
                avg_confidence: 0.88,
                uptime: 99.2,
                last_used: "2024-01-15T10:28:00Z".to_string(),
            },
            capabilities: vec![
                "General Analysis".to_string(),
                "Writing".to_string(),
                "Reasoning".to_string(),
                "Creative Tasks".to_string(),
            ],
            cost_per_token: 0.0,
            privacy_score: 100.0,
        },
        // Add more models as needed...
    ];
    
    Ok(sample_models)
}

fn parse_system_metrics(output: &str) -> Result<SystemMetrics, String> {
    // Parse system metrics from output
    // This would parse actual system status
    
    Ok(SystemMetrics {
        total_models: 5,
        active_models: 4,
        total_storage: "26.5 GB".to_string(),
        total_ram_usage: "38.0 GB".to_string(),
        combined_tokens_per_second: 120.8,
        total_requests: 907,
        avg_confidence: 0.896,
        cost_savings: 1247.50,
        privacy_score: 100.0,
    })
}

async fn load_debate_results(debate_id: &str) -> Result<Option<DebateResult>, String> {
    // Load debate results from file system
    // This would read from UltraMCP's data directories
    
    // For now, return a sample result
    let sample_result = DebateResult {
        id: debate_id.to_string(),
        topic: "Sample debate topic".to_string(),
        consensus: "Based on the analysis, the recommended approach is...".to_string(),
        confidence: 0.87,
        status: "completed".to_string(),
        cost_breakdown: CostBreakdown {
            local: 0.0,
            api: 0.045,
            total: 0.045,
            savings: 0.955,
            savings_percentage: 95.5,
        },
        privacy_score: 85.0,
        participants_used: vec![
            "Qwen 2.5 14B".to_string(),
            "Llama 3.1 8B".to_string(),
            "GPT-4".to_string(),
        ],
        rounds_completed: 3,
        metadata: DebateMetadata {
            total_tokens: 2456,
            avg_response_time: 24.7,
            models_used: {
                let mut map = HashMap::new();
                map.insert("qwen-25-14b".to_string(), 2);
                map.insert("llama-31-8b".to_string(), 2);
                map.insert("gpt-4".to_string(), 1);
                map
            },
            privacy_events: vec![
                "Local processing prioritized".to_string(),
                "Sensitive data identified and isolated".to_string(),
            ],
            optimization_applied: vec![
                "Local model preference".to_string(),
                "Cost-efficient routing".to_string(),
            ],
        },
        timestamp: SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs(),
    };
    
    Ok(Some(sample_result))
}

async fn generate_cost_analytics(time_range: &str) -> Result<CostAnalytics, String> {
    // Generate cost analytics based on system usage
    
    Ok(CostAnalytics {
        current_costs: CostBreakdown {
            local: 0.0,
            api: 0.615,
            total: 0.615,
            savings: 17.835,
            savings_percentage: 96.7,
        },
        model_costs: vec![
            ModelCost {
                model_name: "Qwen 2.5 14B".to_string(),
                model_type: "local".to_string(),
                requests: 147,
                tokens: 58800,
                cost: 0.0,
                avg_cost_per_request: 0.0,
                percentage: 23.5,
            },
            ModelCost {
                model_name: "GPT-4".to_string(),
                model_type: "api".to_string(),
                requests: 23,
                tokens: 15600,
                cost: 0.468,
                avg_cost_per_request: 0.0203,
                percentage: 3.7,
            },
        ],
        savings_calculation: SavingsCalculation {
            total_api_equivalent: 18.45,
            actual_cost: 0.615,
            savings: 17.835,
            savings_percentage: 96.7,
            local_request_count: 662,
            api_request_count: 38,
        },
        optimization_recommendations: vec![
            OptimizationRecommendation {
                id: "1".to_string(),
                recommendation_type: "cost_reduction".to_string(),
                title: "Increase Local Model Usage".to_string(),
                description: "Route 90% of simple queries to local models instead of API models".to_string(),
                potential_savings: 1200.0,
                impact: "high".to_string(),
                effort: "low".to_string(),
            },
        ],
    })
}

fn parse_health_output(output: &str) -> Result<HashMap<String, String>, String> {
    let mut health_status = HashMap::new();
    
    health_status.insert("overall".to_string(), "healthy".to_string());
    health_status.insert("local_models".to_string(), "5 active".to_string());
    health_status.insert("ollama_service".to_string(), "running".to_string());
    health_status.insert("disk_space".to_string(), "sufficient".to_string());
    health_status.insert("memory_usage".to_string(), "normal".to_string());
    
    Ok(health_status)
}

// =============================================================================
// INITIALIZATION
// =============================================================================

pub fn init_ultramcp_commands() -> UltraMCPExecutor {
    UltraMCPExecutor::new()
}