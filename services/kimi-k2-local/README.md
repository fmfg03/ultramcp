# Kimi-K2 Local Integration for UltraMCP

## üß† Model Overview
- **Model**: Kimi-K2 (32B activated, 1T total parameters)
- **Context**: 128K tokens
- **Architecture**: MoE with MLA attention
- **API**: OpenAI/Anthropic compatible

## üîß Installation Options

### Option 1: vLLM (Recommended)
```bash
# Install vLLM
pip install vllm

# Start server
python -m vllm.entrypoints.openai.api_server \
  --model moonshot-ai/Kimi-K2-Instruct \
  --host 0.0.0.0 \
  --port 8011 \
  --max-model-len 128000
```

### Option 2: SGLang
```bash
# Install SGLang
pip install sglang[all]

# Start server
python -m sglang.launch_server \
  --model-path moonshot-ai/Kimi-K2-Instruct \
  --host 0.0.0.0 \
  --port 8011
```

## üê≥ Docker Integration

### Build Custom Image
```dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu22.04

RUN pip install vllm torch transformers

COPY start_kimi.sh /app/
WORKDIR /app

CMD ["./start_kimi.sh"]
```

## üîó UltraMCP Integration

### Add to docker-compose.yml
```yaml
kimi-k2-local:
  build: ./services/kimi-k2-local
  container_name: ultramcp-kimi-k2
  ports:
    - "8011:8011"
  environment:
    - CUDA_VISIBLE_DEVICES=0
  runtime: nvidia
  networks:
    - ultramcp-network
```

### Update CoD Service
```python
# In scripts/cod-service.py
KIMI_LOCAL_URL = "http://kimi-k2-local:8011/v1"

async def use_local_model(prompt):
    headers = {"Content-Type": "application/json"}
    data = {
        "model": "moonshot-ai/Kimi-K2-Instruct",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.6,
        "max_tokens": 4000
    }
    # API call to local Kimi-K2
```

## üìä Performance Expectations
- **Latency**: ~100-500ms per request
- **Throughput**: 10-50 tokens/second
- **Memory**: 20-40GB RAM
- **Context**: Full 128K tokens supported

## üéØ Use Cases in UltraMCP
1. **Local CoD Debates**: Complete offline AI reasoning
2. **Code Analysis**: Long context code understanding
3. **Fallback Model**: When external APIs fail
4. **Privacy**: Sensitive data processing locally

## üîê Security Benefits
- ‚úÖ No data leaves your infrastructure
- ‚úÖ No API key requirements
- ‚úÖ Complete control over model behavior
- ‚úÖ GDPR/compliance friendly