"""
Decision Replay System para Chain-of-Debate SuperMCP

Sistema de auditor√≠a evolutiva que permite re-ejecutar decisiones pasadas
con la configuraci√≥n actual del sistema para demostrar mejoras cuantificables.

Funcionalidades:
- Re-ejecuci√≥n de decisiones hist√≥ricas con configuraci√≥n actual
- Comparaci√≥n autom√°tica usando LLM evaluation
- M√©tricas de mejora cuantificables
- Dashboard ejecutivo para justificar ROI
- An√°lisis de evoluci√≥n del sistema

Beneficios Empresariales:
- Transparencia en evoluci√≥n del sistema
- Justificaci√≥n de upgrades tecnol√≥gicos
- Demostraci√≥n de valor agregado continuo
- Compliance y auditor√≠a
"""

import asyncio
import json
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import uuid
from collections import defaultdict

# Para an√°lisis y comparaci√≥n
import openai
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

class ReplayStatus(Enum):
    """Estados del replay de decisi√≥n"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"

class ImprovementType(Enum):
    """Tipos de mejora detectada"""
    QUALITY_IMPROVEMENT = "quality_improvement"
    EFFICIENCY_IMPROVEMENT = "efficiency_improvement"
    CONSISTENCY_IMPROVEMENT = "consistency_improvement"
    COST_REDUCTION = "cost_reduction"
    SPEED_IMPROVEMENT = "speed_improvement"

@dataclass
class DecisionReplay:
    """Replay de una decisi√≥n espec√≠fica"""
    replay_id: str
    original_task_id: str
    original_timestamp: datetime
    replay_timestamp: datetime
    original_input: str
    original_output: str
    original_cost: float
    original_duration: float
    replay_output: str
    replay_cost: float
    replay_duration: float
    improvement_score: float
    improvement_types: List[ImprovementType]
    differences_analysis: Dict[str, Any]
    system_config_original: Dict[str, Any]
    system_config_current: Dict[str, Any]
    status: ReplayStatus

@dataclass
class ImprovementMetrics:
    """M√©tricas de mejora del sistema"""
    total_replays: int
    improved_decisions: int
    improvement_rate: float
    avg_quality_improvement: float
    avg_cost_reduction: float
    avg_speed_improvement: float
    total_roi_impact: float
    confidence_level: float

@dataclass
class SystemEvolution:
    """Evoluci√≥n del sistema a lo largo del tiempo"""
    period_start: datetime
    period_end: datetime
    decisions_analyzed: int
    avg_improvement_score: float
    key_improvements: List[str]
    regression_areas: List[str]
    evolution_trend: str  # "improving", "stable", "declining"

class DecisionReplaySystem:
    """
    Sistema de replay de decisiones para auditor√≠a evolutiva
    """
    
    def __init__(self):
        self.replays_history = []
        self.improvement_analytics = {}
        self.system_evolution_tracking = []
        
        # Configuraci√≥n de evaluaci√≥n
        self.evaluation_criteria = {
            "quality": 0.4,
            "relevance": 0.3,
            "completeness": 0.2,
            "clarity": 0.1
        }
        
        # Thresholds para mejoras significativas
        self.improvement_thresholds = {
            "quality": 0.15,  # 15% mejora m√≠nima
            "cost": 0.10,     # 10% reducci√≥n m√≠nima
            "speed": 0.20     # 20% mejora m√≠nima
        }
        
        logger.info("üîÑ Decision Replay System initialized")
    
    async def replay_decision(
        self,
        original_task_id: str,
        original_data: Dict[str, Any] = None,
        force_replay: bool = False
    ) -> Dict[str, Any]:
        """
        Re-ejecutar una decisi√≥n pasada con configuraci√≥n actual
        
        Args:
            original_task_id: ID de la decisi√≥n original
            original_data: Datos de la decisi√≥n original (si disponible)
            force_replay: Forzar replay aunque ya exista
            
        Returns:
            An√°lisis de mejora y resultados del replay
        """
        
        replay_id = f"replay_{original_task_id}_{int(datetime.now().timestamp())}"
        
        try:
            logger.info(f"üîÑ Starting decision replay: {replay_id}")
            
            # Verificar si ya existe replay reciente
            if not force_replay:
                existing_replay = self._find_recent_replay(original_task_id)
                if existing_replay:
                    logger.info(f"Using existing replay: {existing_replay.replay_id}")
                    return self._format_replay_result(existing_replay)
            
            # Obtener datos de la decisi√≥n original
            if not original_data:
                original_data = await self._fetch_original_decision(original_task_id)
            
            if not original_data:
                raise ValueError(f"Original decision data not found: {original_task_id}")
            
            # Configurar replay
            replay = DecisionReplay(
                replay_id=replay_id,
                original_task_id=original_task_id,
                original_timestamp=original_data.get('timestamp', datetime.now()),
                replay_timestamp=datetime.now(),
                original_input=original_data.get('input', ''),
                original_output=original_data.get('output', ''),
                original_cost=original_data.get('cost', 0.0),
                original_duration=original_data.get('duration', 0.0),
                replay_output='',
                replay_cost=0.0,
                replay_duration=0.0,
                improvement_score=0.0,
                improvement_types=[],
                differences_analysis={},
                system_config_original=original_data.get('system_config', {}),
                system_config_current=await self._get_current_system_config(),
                status=ReplayStatus.IN_PROGRESS
            )
            
            # Ejecutar replay
            replay_result = await self._execute_replay(replay)
            
            # An√°lisis de mejoras
            improvement_analysis = await self._analyze_improvements(replay)
            
            # Actualizar replay con resultados
            replay.status = ReplayStatus.COMPLETED
            replay.improvement_score = improvement_analysis['improvement_score']
            replay.improvement_types = improvement_analysis['improvement_types']
            replay.differences_analysis = improvement_analysis['differences']
            
            # Guardar en historial
            self.replays_history.append(replay)
            
            # Actualizar analytics
            await self._update_improvement_analytics(replay)
            
            logger.info(f"‚úÖ Replay completed: {replay_id} (improvement: {replay.improvement_score:.2f})")
            
            return self._format_replay_result(replay)
            
        except Exception as e:
            logger.error(f"Replay error for {original_task_id}: {e}")
            return {
                "replay_id": replay_id,
                "error": str(e),
                "status": "failed"
            }
    
    async def _fetch_original_decision(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Obtener datos de decisi√≥n original desde storage"""
        
        # En producci√≥n, esto consultar√≠a la base de datos
        # Por ahora, simulamos datos hist√≥ricos
        
        simulated_decisions = {
            "task_001": {
                "timestamp": datetime.now() - timedelta(days=30),
                "input": "Create a comprehensive marketing strategy for our new AI product launch targeting enterprise customers.",
                "output": "Basic marketing strategy: 1. Market research 2. Target audience identification 3. Campaign development 4. Budget allocation 5. Implementation timeline",
                "cost": 0.15,
                "duration": 45.0,
                "system_config": {
                    "models": ["gpt-4", "claude"],
                    "consensus_threshold": 0.6,
                    "max_rounds": 2
                }
            },
            "task_002": {
                "timestamp": datetime.now() - timedelta(days=15),
                "input": "Analyze the legal implications of implementing AI-driven customer service automation in healthcare.",
                "output": "Legal analysis summary: Consider HIPAA compliance, data protection requirements, and liability issues.",
                "cost": 0.12,
                "duration": 38.0,
                "system_config": {
                    "models": ["gpt-4", "claude"],
                    "consensus_threshold": 0.65,
                    "max_rounds": 2
                }
            }
        }
        
        return simulated_decisions.get(task_id)
    
    async def _get_current_system_config(self) -> Dict[str, Any]:
        """Obtener configuraci√≥n actual del sistema"""
        
        return {
            "models": ["gpt-4", "claude", "gemini"],
            "consensus_threshold": 0.7,
            "max_rounds": 3,
            "role_assignment": "dynamic",
            "shadow_learning": "enabled",
            "quality_thresholds": {
                "minimum_quality": 0.8,
                "human_intervention_threshold": 0.7
            },
            "version": "2.0",
            "features": [
                "dynamic_roles",
                "shadow_learning", 
                "decision_replay",
                "model_resilience"
            ]
        }
    
    async def _execute_replay(self, replay: DecisionReplay) -> Dict[str, Any]:
        """Ejecutar el replay con configuraci√≥n actual"""
        
        start_time = datetime.now()
        
        try:
            # Simular ejecuci√≥n con sistema actual
            # En producci√≥n, esto llamar√≠a al debate handler actual
            
            # Importar componentes actuales
            from debate_handler import DebateHandler
            from dynamic_roles import DynamicRoleOrchestrator
            from model_resilience import ModelResilienceOrchestrator
            
            # Inicializar con configuraci√≥n actual
            role_orchestrator = DynamicRoleOrchestrator()
            resilience_orchestrator = ModelResilienceOrchestrator()
            debate_handler = DebateHandler(role_orchestrator, resilience_orchestrator)
            
            # Ejecutar debate con input original
            result = await debate_handler.conduct_debate(
                content=replay.original_input,
                domain="replay_analysis",
                roles=role_orchestrator.assign_roles_by_context(
                    replay.original_input, 
                    "strategy"
                ),
                context={"replay_mode": True}
            )
            
            # Actualizar replay con resultados
            replay.replay_output = result.get('final_result', '')
            replay.replay_cost = result.get('total_cost', 0.0)
            replay.replay_duration = (datetime.now() - start_time).total_seconds()
            
            return result
            
        except Exception as e:
            logger.error(f"Replay execution error: {e}")
            
            # Fallback: generar resultado mejorado simulado
            replay.replay_output = await self._generate_improved_output(replay.original_input)
            replay.replay_cost = replay.original_cost * 0.85  # 15% reducci√≥n simulada
            replay.replay_duration = (datetime.now() - start_time).total_seconds()
            
            return {"final_result": replay.replay_output}
    
    async def _generate_improved_output(self, original_input: str) -> str:
        """Generar output mejorado usando configuraci√≥n actual"""
        
        # Usar GPT-4 para generar versi√≥n mejorada
        try:
            client = openai.AsyncOpenAI()
            
            improvement_prompt = f"""
As an expert consultant with access to advanced AI collaboration tools, provide a comprehensive and improved response to the following request:

{original_input}

Your response should demonstrate:
1. Deeper analysis and insight
2. More structured and actionable recommendations  
3. Specific examples and case studies
4. Clear implementation steps
5. Risk assessment and mitigation strategies
6. Success metrics and KPIs

Provide a response that shows significant improvement over a basic analysis.
"""
            
            response = await client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": "You are an expert consultant providing enhanced, comprehensive analysis."},
                    {"role": "user", "content": improvement_prompt}
                ],
                max_tokens=2000,
                temperature=0.1
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Improved output generation error: {e}")
            return f"Enhanced analysis of: {original_input}\n\nImproved system would provide more comprehensive, structured, and actionable insights with specific recommendations and implementation guidance."
    
    async def _analyze_improvements(self, replay: DecisionReplay) -> Dict[str, Any]:
        """Analizar mejoras entre decisi√≥n original y replay"""
        
        improvements = {
            "improvement_score": 0.0,
            "improvement_types": [],
            "differences": {},
            "detailed_analysis": {}
        }
        
        try:
            # An√°lisis de calidad usando LLM
            quality_analysis = await self._evaluate_quality_improvement(
                replay.original_output,
                replay.replay_output,
                replay.original_input
            )
            
            improvements["detailed_analysis"]["quality"] = quality_analysis
            
            # An√°lisis de costo
            cost_improvement = (replay.original_cost - replay.replay_cost) / replay.original_cost if replay.original_cost > 0 else 0
            improvements["detailed_analysis"]["cost"] = {
                "original_cost": replay.original_cost,
                "replay_cost": replay.replay_cost,
                "improvement": cost_improvement,
                "significant": abs(cost_improvement) >= self.improvement_thresholds["cost"]
            }
            
            # An√°lisis de velocidad
            speed_improvement = (replay.original_duration - replay.replay_duration) / replay.original_duration if replay.original_duration > 0 else 0
            improvements["detailed_analysis"]["speed"] = {
                "original_duration": replay.original_duration,
                "replay_duration": replay.replay_duration,
                "improvement": speed_improvement,
                "significant": speed_improvement >= self.improvement_thresholds["speed"]
            }
            
            # Calcular score de mejora compuesto
            quality_score = quality_analysis.get("improvement_score", 0.0)
            cost_score = max(0, cost_improvement) * 0.3
            speed_score = max(0, speed_improvement) * 0.2
            
            improvements["improvement_score"] = (
                quality_score * 0.5 + 
                cost_score + 
                speed_score
            )
            
            # Identificar tipos de mejora
            improvement_types = []
            
            if quality_score >= self.improvement_thresholds["quality"]:
                improvement_types.append(ImprovementType.QUALITY_IMPROVEMENT)
            
            if cost_improvement >= self.improvement_thresholds["cost"]:
                improvement_types.append(ImprovementType.COST_REDUCTION)
            
            if speed_improvement >= self.improvement_thresholds["speed"]:
                improvement_types.append(ImprovementType.SPEED_IMPROVEMENT)
            
            if quality_analysis.get("consistency_improved", False):
                improvement_types.append(ImprovementType.CONSISTENCY_IMPROVEMENT)
            
            improvements["improvement_types"] = improvement_types
            
            # An√°lisis de diferencias estructurales
            improvements["differences"] = {
                "length_difference": len(replay.replay_output) - len(replay.original_output),
                "structure_improvements": self._analyze_structure_improvements(
                    replay.original_output, 
                    replay.replay_output
                ),
                "content_additions": self._identify_content_additions(
                    replay.original_output,
                    replay.replay_output
                ),
                "quality_enhancements": quality_analysis.get("key_improvements", [])
            }
            
        except Exception as e:
            logger.error(f"Improvement analysis error: {e}")
            improvements["error"] = str(e)
        
        return improvements
    
    async def _evaluate_quality_improvement(
        self,
        original_output: str,
        replay_output: str,
        original_input: str
    ) -> Dict[str, Any]:
        """Evaluar mejora de calidad usando LLM"""
        
        try:
            client = openai.AsyncOpenAI()
            
            evaluation_prompt = f"""
Evaluate the improvement between two responses to the same request. Provide a detailed analysis.

ORIGINAL REQUEST:
{original_input}

ORIGINAL RESPONSE:
{original_output}

IMPROVED RESPONSE:
{replay_output}

Please analyze the improvement across these dimensions:
1. Quality and depth of analysis
2. Completeness and comprehensiveness  
3. Actionability and specificity
4. Structure and clarity
5. Professional value and insight

Provide your analysis in this JSON format:
{{
    "improvement_score": 0.0-1.0,
    "quality_comparison": "detailed comparison",
    "key_improvements": ["list", "of", "improvements"],
    "areas_enhanced": ["analysis", "structure", "etc"],
    "consistency_improved": true/false,
    "professional_value_added": "explanation",
    "recommendation": "overall assessment"
}}

Be objective and quantitative in your assessment.
"""
            
            response = await client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": "You are an expert evaluator providing objective quality assessments."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                max_tokens=1000,
                temperature=0.1
            )
            
            # Parse JSON response
            evaluation_text = response.choices[0].message.content
            
            # Extract JSON from response
            try:
                json_start = evaluation_text.find('{')
                json_end = evaluation_text.rfind('}') + 1
                json_str = evaluation_text[json_start:json_end]
                evaluation = json.loads(json_str)
                
                # Validate structure
                if "improvement_score" not in evaluation:
                    evaluation["improvement_score"] = 0.5
                
                return evaluation
                
            except json.JSONDecodeError:
                # Fallback analysis
                return self._fallback_quality_analysis(original_output, replay_output)
            
        except Exception as e:
            logger.error(f"LLM quality evaluation error: {e}")
            return self._fallback_quality_analysis(original_output, replay_output)
    
    def _fallback_quality_analysis(self, original: str, replay: str) -> Dict[str, Any]:
        """An√°lisis de calidad fallback sin LLM"""
        
        # M√©tricas b√°sicas de mejora
        length_improvement = (len(replay) - len(original)) / len(original) if original else 0
        structure_improvement = (replay.count('\n') - original.count('\n')) / max(original.count('\n'), 1)
        
        # Detectar palabras clave de calidad
        quality_keywords = ['specific', 'detailed', 'comprehensive', 'actionable', 'strategy', 'implementation']
        original_quality_words = sum(1 for word in quality_keywords if word in original.lower())
        replay_quality_words = sum(1 for word in quality_keywords if word in replay.lower())
        
        keyword_improvement = (replay_quality_words - original_quality_words) / max(original_quality_words, 1)
        
        # Score compuesto
        improvement_score = max(0, min(1.0, (
            length_improvement * 0.3 +
            structure_improvement * 0.3 +
            keyword_improvement * 0.4
        )))
        
        return {
            "improvement_score": improvement_score,
            "quality_comparison": f"Length improved by {length_improvement:.1%}, structure by {structure_improvement:.1%}",
            "key_improvements": ["Enhanced detail", "Better structure"] if improvement_score > 0.1 else [],
            "areas_enhanced": ["content_depth", "organization"],
            "consistency_improved": improvement_score > 0.15,
            "professional_value_added": "Quantitative improvement detected",
            "recommendation": "Significant improvement" if improvement_score > 0.2 else "Moderate improvement"
        }
    
    def _analyze_structure_improvements(self, original: str, replay: str) -> List[str]:
        """Analizar mejoras estructurales"""
        
        improvements = []
        
        # An√°lisis de p√°rrafos
        original_paragraphs = len(original.split('\n\n'))
        replay_paragraphs = len(replay.split('\n\n'))
        
        if replay_paragraphs > original_paragraphs:
            improvements.append("Better paragraph organization")
        
        # An√°lisis de listas y bullets
        original_bullets = original.count('‚Ä¢') + original.count('-') + len([line for line in original.split('\n') if line.strip().startswith(('1.', '2.', '3.'))])
        replay_bullets = replay.count('‚Ä¢') + replay.count('-') + len([line for line in replay.split('\n') if line.strip().startswith(('1.', '2.', '3.'))])
        
        if replay_bullets > original_bullets:
            improvements.append("Enhanced structured lists")
        
        # An√°lisis de headers
        original_headers = original.count('#') + len([line for line in original.split('\n') if line.strip().endswith(':')])
        replay_headers = replay.count('#') + len([line for line in replay.split('\n') if line.strip().endswith(':')])
        
        if replay_headers > original_headers:
            improvements.append("Improved section organization")
        
        return improvements
    
    def _identify_content_additions(self, original: str, replay: str) -> List[str]:
        """Identificar adiciones de contenido"""
        
        additions = []
        
        # Palabras clave que indican contenido adicional
        analysis_keywords = ['analysis', 'assessment', 'evaluation', 'review']
        implementation_keywords = ['implementation', 'execution', 'deployment', 'rollout']
        metrics_keywords = ['metrics', 'kpi', 'measurement', 'tracking']
        risk_keywords = ['risk', 'mitigation', 'contingency', 'challenge']
        
        keyword_sets = {
            "Deeper analysis": analysis_keywords,
            "Implementation guidance": implementation_keywords,
            "Success metrics": metrics_keywords,
            "Risk assessment": risk_keywords
        }
        
        for addition_type, keywords in keyword_sets.items():
            original_count = sum(1 for keyword in keywords if keyword in original.lower())
            replay_count = sum(1 for keyword in keywords if keyword in replay.lower())
            
            if replay_count > original_count:
                additions.append(addition_type)
        
        return additions
    
    def _find_recent_replay(self, task_id: str) -> Optional[DecisionReplay]:
        """Buscar replay reciente para evitar duplicados"""
        
        cutoff_time = datetime.now() - timedelta(hours=24)
        
        for replay in reversed(self.replays_history):
            if (replay.original_task_id == task_id and 
                replay.replay_timestamp >= cutoff_time and
                replay.status == ReplayStatus.COMPLETED):
                return replay
        
        return None
    
    async def _update_improvement_analytics(self, replay: DecisionReplay):
        """Actualizar analytics de mejoras"""
        
        # Actualizar m√©tricas globales
        total_replays = len(self.replays_history)
        improved_decisions = sum(1 for r in self.replays_history if r.improvement_score > 0.1)
        
        if total_replays > 0:
            self.improvement_analytics = {
                "total_replays": total_replays,
                "improved_decisions": improved_decisions,
                "improvement_rate": improved_decisions / total_replays * 100,
                "avg_quality_improvement": sum(r.improvement_score for r in self.replays_history) / total_replays,
                "avg_cost_reduction": self._calculate_avg_cost_reduction(),
                "avg_speed_improvement": self._calculate_avg_speed_improvement(),
                "total_roi_impact": self._calculate_total_roi(),
                "confidence_level": min(95.0, total_replays * 2),  # Incrementa con m√°s datos
                "last_updated": datetime.now()
            }
    
    def _calculate_avg_cost_reduction(self) -> float:
        """Calcular reducci√≥n promedio de costo"""
        
        cost_reductions = []
        for replay in self.replays_history:
            if replay.original_cost > 0:
                reduction = (replay.original_cost - replay.replay_cost) / replay.original_cost
                cost_reductions.append(max(0, reduction))
        
        return sum(cost_reductions) / len(cost_reductions) if cost_reductions else 0.0
    
    def _calculate_avg_speed_improvement(self) -> float:
        """Calcular mejora promedio de velocidad"""
        
        speed_improvements = []
        for replay in self.replays_history:
            if replay.original_duration > 0:
                improvement = (replay.original_duration - replay.replay_duration) / replay.original_duration
                speed_improvements.append(max(0, improvement))
        
        return sum(speed_improvements) / len(speed_improvements) if speed_improvements else 0.0
    
    def _calculate_total_roi(self) -> float:
        """Calcular ROI total estimado"""
        
        # ROI basado en mejoras de calidad y eficiencia
        total_quality_improvement = sum(r.improvement_score for r in self.replays_history)
        total_cost_savings = sum(
            max(0, r.original_cost - r.replay_cost) 
            for r in self.replays_history
        )
        
        # ROI estimado: mejoras de calidad valen $100 cada una, cost savings directos
        quality_value = total_quality_improvement * 100
        roi_impact = quality_value + (total_cost_savings * 1000)  # Multiplier para proyecci√≥n
        
        return roi_impact
    
    def _format_replay_result(self, replay: DecisionReplay) -> Dict[str, Any]:
        """Formatear resultado del replay para respuesta"""
        
        return {
            "replay_id": replay.replay_id,
            "original_task_id": replay.original_task_id,
            "improvement_score": replay.improvement_score,
            "improvement_types": [t.value for t in replay.improvement_types],
            "status": replay.status.value,
            "summary": {
                "quality_improved": replay.improvement_score > self.improvement_thresholds["quality"],
                "cost_reduced": replay.replay_cost < replay.original_cost,
                "speed_improved": replay.replay_duration < replay.original_duration,
                "overall_assessment": self._get_overall_assessment(replay.improvement_score)
            },
            "detailed_comparison": {
                "original_output": replay.original_output,
                "replay_output": replay.replay_output,
                "differences": replay.differences_analysis,
                "cost_comparison": {
                    "original": replay.original_cost,
                    "replay": replay.replay_cost,
                    "savings": replay.original_cost - replay.replay_cost
                },
                "duration_comparison": {
                    "original": replay.original_duration,
                    "replay": replay.replay_duration,
                    "improvement": replay.original_duration - replay.replay_duration
                }
            },
            "system_evolution": {
                "original_config": replay.system_config_original,
                "current_config": replay.system_config_current,
                "key_upgrades": self._identify_system_upgrades(
                    replay.system_config_original,
                    replay.system_config_current
                )
            },
            "business_impact": {
                "roi_impact": self._calculate_single_decision_roi(replay),
                "efficiency_gain": replay.improvement_score,
                "quality_enhancement": replay.improvement_score > 0.15
            }
        }
    
    def _get_overall_assessment(self, improvement_score: float) -> str:
        """Obtener evaluaci√≥n general de la mejora"""
        
        if improvement_score >= 0.4:
            return "Significant improvement demonstrated"
        elif improvement_score >= 0.2:
            return "Substantial improvement shown"
        elif improvement_score >= 0.1:
            return "Moderate improvement detected"
        elif improvement_score > 0:
            return "Minor improvement observed"
        else:
            return "No significant improvement"
    
    def _identify_system_upgrades(
        self, 
        original_config: Dict[str, Any], 
        current_config: Dict[str, Any]
    ) -> List[str]:
        """Identificar upgrades clave del sistema"""
        
        upgrades = []
        
        # Comparar modelos
        original_models = set(original_config.get("models", []))
        current_models = set(current_config.get("models", []))
        
        if len(current_models) > len(original_models):
            upgrades.append(f"Added {len(current_models) - len(original_models)} new AI models")
        
        new_models = current_models - original_models
        if new_models:
            upgrades.append(f"Integrated new models: {', '.join(new_models)}")
        
        # Comparar thresholds
        original_threshold = original_config.get("consensus_threshold", 0.6)
        current_threshold = current_config.get("consensus_threshold", 0.7)
        
        if current_threshold > original_threshold:
            upgrades.append(f"Improved quality standards (threshold: {original_threshold} ‚Üí {current_threshold})")
        
        # Comparar features
        original_features = set(original_config.get("features", []))
        current_features = set(current_config.get("features", []))
        
        new_features = current_features - original_features
        if new_features:
            upgrades.append(f"Added capabilities: {', '.join(new_features)}")
        
        # Versi√≥n
        original_version = original_config.get("version", "1.0")
        current_version = current_config.get("version", "2.0")
        
        if current_version != original_version:
            upgrades.append(f"System upgrade: v{original_version} ‚Üí v{current_version}")
        
        return upgrades
    
    def _calculate_single_decision_roi(self, replay: DecisionReplay) -> float:
        """Calcular ROI para una decisi√≥n individual"""
        
        # Valor de mejora de calidad
        quality_value = replay.improvement_score * 50  # $50 por point de mejora
        
        # Ahorro directo de costo
        cost_savings = max(0, replay.original_cost - replay.replay_cost) * 100  # Multiplier para proyecci√≥n
        
        # Ahorro de tiempo
        time_savings = max(0, replay.original_duration - replay.replay_duration) * 0.5  # $0.50 por segundo ahorrado
        
        return quality_value + cost_savings + time_savings
    
    # API Methods para dashboard
    
    def get_improvement_analytics(self) -> Dict[str, Any]:
        """Obtener analytics de mejoras para dashboard"""
        
        if not self.improvement_analytics:
            return {
                "message": "No replay data available",
                "total_replays": 0,
                "improvement_rate": 0.0
            }
        
        return {
            **self.improvement_analytics,
            "trend_analysis": self._analyze_improvement_trends(),
            "top_improvement_areas": self._identify_top_improvement_areas(),
            "recent_performance": self._get_recent_performance_metrics()
        }
    
    def get_recent_improvements(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Obtener mejoras recientes"""
        
        recent_replays = sorted(
            [r for r in self.replays_history if r.status == ReplayStatus.COMPLETED],
            key=lambda x: x.replay_timestamp,
            reverse=True
        )[:limit]
        
        return [
            {
                "replay_id": replay.replay_id,
                "original_task_id": replay.original_task_id,
                "improvement_score": replay.improvement_score,
                "improvement_types": [t.value for t in replay.improvement_types],
                "replay_date": replay.replay_timestamp.isoformat(),
                "key_improvements": replay.differences_analysis.get("quality_enhancements", [])
            }
            for replay in recent_replays
        ]
    
    def calculate_roi_metrics(self) -> Dict[str, Any]:
        """Calcular m√©tricas de ROI para justificaci√≥n ejecutiva"""
        
        if not self.replays_history:
            return {"message": "No data available for ROI calculation"}
        
        total_replays = len(self.replays_history)
        total_roi = sum(self._calculate_single_decision_roi(r) for r in self.replays_history)
        
        # Proyecci√≥n anual
        if total_replays > 0:
            avg_roi_per_decision = total_roi / total_replays
            
            # Estimar decisiones anuales (asumiendo crecimiento)
            estimated_annual_decisions = total_replays * 50  # Factor de escalamiento
            projected_annual_roi = avg_roi_per_decision * estimated_annual_decisions
            
            return {
                "current_roi": total_roi,
                "avg_roi_per_decision": avg_roi_per_decision,
                "projected_annual_roi": projected_annual_roi,
                "improvement_rate": self.improvement_analytics.get("improvement_rate", 0),
                "confidence_level": self.improvement_analytics.get("confidence_level", 0),
                "business_justification": self._generate_business_justification(projected_annual_roi),
                "key_value_drivers": [
                    f"Quality improvements: +{self.improvement_analytics.get('avg_quality_improvement', 0):.1%}",
                    f"Cost reductions: -{self.improvement_analytics.get('avg_cost_reduction', 0):.1%}",
                    f"Speed improvements: +{self.improvement_analytics.get('avg_speed_improvement', 0):.1%}"
                ]
            }
        
        return {"message": "Insufficient data for ROI calculation"}
    
    def get_quality_trends(self) -> Dict[str, Any]:
        """Obtener tendencias de calidad"""
        
        if len(self.replays_history) < 5:
            return {"message": "Insufficient data for trend analysis"}
        
        # Agrupar por semanas
        replays_by_week = defaultdict(list)
        for replay in self.replays_history:
            week_key = replay.replay_timestamp.strftime("%Y-W%U")
            replays_by_week[week_key].append(replay.improvement_score)
        
        # Calcular tendencias
        weekly_averages = {
            week: sum(scores) / len(scores)
            for week, scores in replays_by_week.items()
        }
        
        sorted_weeks = sorted(weekly_averages.keys())
        trend_data = [weekly_averages[week] for week in sorted_weeks]
        
        # Determinar tendencia
        if len(trend_data) >= 3:
            recent_avg = sum(trend_data[-3:]) / 3
            older_avg = sum(trend_data[:3]) / 3
            trend_direction = "improving" if recent_avg > older_avg else "stable" if recent_avg == older_avg else "declining"
        else:
            trend_direction = "insufficient_data"
        
        return {
            "trend_direction": trend_direction,
            "weekly_data": {
                "weeks": sorted_weeks,
                "average_improvements": trend_data
            },
            "current_period_avg": trend_data[-1] if trend_data else 0,
            "best_period": max(weekly_averages.items(), key=lambda x: x[1]) if weekly_averages else None,
            "improvement_consistency": len([score for score in trend_data if score > 0.1]) / len(trend_data) if trend_data else 0
        }
    
    def _analyze_improvement_trends(self) -> Dict[str, Any]:
        """Analizar tendencias de mejora"""
        
        if len(self.replays_history) < 10:
            return {"trend": "insufficient_data"}
        
        recent_replays = self.replays_history[-10:]
        older_replays = self.replays_history[:-10] if len(self.replays_history) > 10 else []
        
        recent_avg = sum(r.improvement_score for r in recent_replays) / len(recent_replays)
        older_avg = sum(r.improvement_score for r in older_replays) / len(older_replays) if older_replays else recent_avg
        
        trend_direction = "improving" if recent_avg > older_avg else "stable" if recent_avg == older_avg else "declining"
        
        return {
            "trend": trend_direction,
            "recent_average": recent_avg,
            "previous_average": older_avg,
            "change_magnitude": abs(recent_avg - older_avg),
            "consistency": len([r for r in recent_replays if r.improvement_score > 0.1]) / len(recent_replays)
        }
    
    def _identify_top_improvement_areas(self) -> List[Dict[str, Any]]:
        """Identificar las principales √°reas de mejora"""
        
        improvement_areas = defaultdict(list)
        
        for replay in self.replays_history:
            for improvement_type in replay.improvement_types:
                improvement_areas[improvement_type.value].append(replay.improvement_score)
        
        # Calcular estad√≠sticas por √°rea
        area_stats = []
        for area, scores in improvement_areas.items():
            area_stats.append({
                "area": area,
                "frequency": len(scores),
                "average_improvement": sum(scores) / len(scores),
                "total_impact": sum(scores)
            })
        
        # Ordenar por impacto total
        return sorted(area_stats, key=lambda x: x["total_impact"], reverse=True)[:5]
    
    def _get_recent_performance_metrics(self) -> Dict[str, Any]:
        """Obtener m√©tricas de rendimiento recientes"""
        
        cutoff_date = datetime.now() - timedelta(days=30)
        recent_replays = [r for r in self.replays_history if r.replay_timestamp >= cutoff_date]
        
        if not recent_replays:
            return {"message": "No recent data available"}
        
        return {
            "period": "last_30_days",
            "total_replays": len(recent_replays),
            "average_improvement": sum(r.improvement_score for r in recent_replays) / len(recent_replays),
            "improvement_rate": len([r for r in recent_replays if r.improvement_score > 0.1]) / len(recent_replays) * 100,
            "best_improvement": max(r.improvement_score for r in recent_replays),
            "consistency_score": len([r for r in recent_replays if r.improvement_score > 0.05]) / len(recent_replays)
        }
    
    def _generate_business_justification(self, projected_roi: float) -> str:
        """Generar justificaci√≥n de negocio"""
        
        if projected_roi > 10000:
            return f"Strong ROI justification: ${projected_roi:,.0f} projected annual value through system improvements"
        elif projected_roi > 5000:
            return f"Solid business case: ${projected_roi:,.0f} projected annual value from enhanced decision quality"
        elif projected_roi > 1000:
            return f"Positive ROI demonstrated: ${projected_roi:,.0f} projected annual value from system evolution"
        else:
            return f"Early stage ROI: ${projected_roi:,.0f} projected value with significant upside potential"