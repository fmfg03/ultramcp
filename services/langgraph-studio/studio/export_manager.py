"""
Export Manager para LangGraph Studio
Maneja la exportación de grafos, trazas y visualizaciones
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class ExportManager:
    """Maneja todas las exportaciones de LangGraph Studio"""
    
    def __init__(self, export_path: str = "./langgraph_system/studio/studio_exports/"):
        self.export_path = Path(export_path)
        self.export_path.mkdir(parents=True, exist_ok=True)
        
        # Crear subdirectorios
        (self.export_path / "graphs").mkdir(exist_ok=True)
        (self.export_path / "traces").mkdir(exist_ok=True)
        (self.export_path / "screenshots").mkdir(exist_ok=True)
        (self.export_path / "reports").mkdir(exist_ok=True)
    
    def export_complete_system_graph(self) -> Dict[str, str]:
        """Exporta el grafo completo del sistema en múltiples formatos"""
        exports = {}
        
        # 1. Mermaid completo
        mermaid_content = self._generate_complete_mermaid()
        mermaid_path = self.export_path / "graphs" / "mcp_complete_system.mmd"
        with open(mermaid_path, 'w') as f:
            f.write(mermaid_content)
        exports["mermaid"] = str(mermaid_path)
        
        # 2. Mermaid para pitch deck
        pitch_content = self._generate_pitch_deck_mermaid()
        pitch_path = self.export_path / "graphs" / "mcp_pitch_deck.mmd"
        with open(pitch_path, 'w') as f:
            f.write(pitch_content)
        exports["pitch_deck"] = str(pitch_path)
        
        # 3. DOT para Graphviz
        dot_content = self._generate_dot_graph()
        dot_path = self.export_path / "graphs" / "mcp_system.dot"
        with open(dot_path, 'w') as f:
            f.write(dot_content)
        exports["dot"] = str(dot_path)
        
        # 4. JSON schema
        schema_content = self._generate_json_schema()
        schema_path = self.export_path / "graphs" / "mcp_schema.json"
        with open(schema_path, 'w') as f:
            json.dump(schema_content, f, indent=2)
        exports["schema"] = str(schema_path)
        
        # 5. Manifest de exportación
        manifest = {
            "export_timestamp": datetime.now().isoformat(),
            "system_version": "1.0.0",
            "exports": exports,
            "metadata": {
                "total_nodes": 11,
                "total_edges": 13,
                "features": [
                    "Reasoning Shell",
                    "Reward Shell", 
                    "Contradiction Analysis",
                    "Intelligent Retry",
                    "Local LLMs",
                    "Langwatch Integration"
                ]
            }
        }
        
        manifest_path = self.export_path / "export_manifest.json"
        with open(manifest_path, 'w') as f:
            json.dump(manifest, f, indent=2)
        exports["manifest"] = str(manifest_path)
        
        logger.info(f"System graphs exported to: {self.export_path}")
        return exports
    
    def _generate_complete_mermaid(self) -> str:
        """Genera diagrama Mermaid completo del sistema"""
        return """graph TD
    %% MCP SYSTEM - Complete Architecture
    %% Generated by LangGraph Studio Export Manager
    
    %% Styling
    classDef startEnd fill:#e1f5fe,stroke:#01579b,stroke-width:3px,color:#fff
    classDef reasoning fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#fff
    classDef execution fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px,color:#fff
    classDef evaluation fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#fff
    classDef decision fill:#fce4ec,stroke:#880e4f,stroke-width:2px,color:#fff
    classDef memory fill:#f1f8e9,stroke:#33691e,stroke-width:2px,color:#fff
    classDef llm fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#fff
    
    %% Main Flow Nodes
    START([🚀 START]):::startEnd
    INIT[🔧 Initialize Session<br/>📋 Config & Memory]:::memory
    HEALTH[❤️ Health Check<br/>🔍 System Status]:::evaluation
    REASON[🧠 Enhanced Reasoning<br/>📝 Task Analysis & Planning]:::reasoning
    SELECT[🎯 Adaptive Selection<br/>🤖 Auto-detect Best Model]:::decision
    EXECUTE[⚡ Execute LLM<br/>🔥 Local Models + Langwatch]:::llm
    EVALUATE[📊 Enhanced Reward<br/>🏆 Multi-dimensional Scoring]:::evaluation
    CONTRADICTION[🔥 Contradiction Analysis<br/>💡 Failure Detection & Learning]:::decision
    RETRY[🔄 Intelligent Retry<br/>🎲 Strategy Selection]:::decision
    FINALIZE[✅ Finalize Results<br/>💾 Save & Cleanup]:::memory
    END([🏁 END]):::startEnd
    
    %% Local LLM Nodes
    MISTRAL[🧙‍♂️ Mistral Local<br/>General Reasoning]:::llm
    LLAMA[🦙 LLaMA Local<br/>Text Generation]:::llm
    DEEPSEEK[🔬 DeepSeek Local<br/>Math & Logic]:::llm
    
    %% Main Flow
    START --> INIT
    INIT --> HEALTH
    HEALTH --> REASON
    REASON --> SELECT
    SELECT --> EXECUTE
    EXECUTE --> EVALUATE
    EVALUATE --> CONTRADICTION
    CONTRADICTION --> RETRY
    RETRY --> FINALIZE
    FINALIZE --> END
    
    %% Model Selection Flow
    SELECT -.->|Math/Logic| DEEPSEEK
    SELECT -.->|General| MISTRAL
    SELECT -.->|Text Gen| LLAMA
    DEEPSEEK -.-> EXECUTE
    MISTRAL -.-> EXECUTE
    LLAMA -.-> EXECUTE
    
    %% Conditional Edges
    HEALTH -->|❌ System Unhealthy| END
    EVALUATE -->|⭐ Score >= 0.8| FINALIZE
    EVALUATE -->|📉 Score < 0.8| CONTRADICTION
    CONTRADICTION -->|🔥 Apply Contradiction| REASON
    CONTRADICTION -->|✅ No Contradiction Needed| RETRY
    RETRY -->|🔄 Retry with Same Model| SELECT
    RETRY -->|🔀 Try Different Model| SELECT
    RETRY -->|🛑 Max Retries Reached| FINALIZE
    
    %% Langwatch Integration (invisible connections)
    EXECUTE -.->|📊 Track Metrics| EVALUATE
    CONTRADICTION -.->|📈 Effectiveness Analysis| RETRY
    
    %% Metadata
    %% Total Nodes: 14
    %% Total Edges: 16
    %% Features: Reasoning, Reward, Contradiction, Retry, Memory, Local LLMs, Langwatch
    %% Models: Mistral, LLaMA, DeepSeek (.gguf format)
    %% Monitoring: Real-time debugging, Session tracking, Performance metrics
"""
    
    def _generate_pitch_deck_mermaid(self) -> str:
        """Genera versión simplificada para pitch deck"""
        return """---
title: "Agentius MCP - Autonomous Agent Runtime"
---
graph LR
    %% AGENTIUS MCP - Pitch Deck Version
    %% Simplified flow for presentations
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:4px,color:#fff,font-size:14px
    classDef intelligence fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#fff,font-size:14px
    classDef execution fill:#e8f5e8,stroke:#388e3c,stroke-width:3px,color:#fff,font-size:14px
    classDef output fill:#fff3e0,stroke:#f57c00,stroke-width:4px,color:#fff,font-size:14px
    
    %% Main Components
    INPUT[📝 User Input<br/>Natural Language]:::input
    REASONING[🧠 AI Reasoning<br/>Task Planning]:::intelligence
    MODELS[🤖 Local LLMs<br/>Mistral • LLaMA • DeepSeek]:::execution
    CONTRADICTION[🔥 Contradiction Engine<br/>Learns from Failures]:::intelligence
    OUTPUT[✨ Optimized Result<br/>High Quality Output]:::output
    
    %% Flow
    INPUT --> REASONING
    REASONING --> MODELS
    MODELS --> OUTPUT
    
    %% Feedback Loop
    OUTPUT -.->|❌ Low Quality| CONTRADICTION
    CONTRADICTION -.->|🔄 Force New Approach| REASONING
    
    %% Key Features (annotations)
    REASONING -.->|"Auto-detects best model<br/>for each task type"| MODELS
    MODELS -.->|"100% Local Processing<br/>No API dependencies"| OUTPUT
    CONTRADICTION -.->|"Explicit contradiction<br/>transforms failures into learning"| REASONING
    
    %% Monitoring Layer
    LANGWATCH[📊 Langwatch<br/>Real-time Analytics]:::intelligence
    MODELS -.-> LANGWATCH
    LANGWATCH -.-> OUTPUT
"""
    
    def _generate_dot_graph(self) -> str:
        """Genera grafo en formato DOT para Graphviz"""
        return """digraph MCP_System {
    // MCP System - Technical Architecture
    // Generated for technical documentation
    
    // Graph settings
    rankdir=TD;
    node [shape=box, style=filled, fontname="Arial"];
    edge [fontname="Arial", fontsize=10];
    
    // Node definitions with colors
    start [label="START", shape=ellipse, fillcolor="#4CAF50"];
    init [label="Initialize\\nSession", fillcolor="#E3F2FD"];
    health [label="Health\\nCheck", fillcolor="#FFF3E0"];
    reasoning [label="Reasoning\\nShell", fillcolor="#F3E5F5"];
    selection [label="Model\\nSelection", fillcolor="#FCE4EC", shape=diamond];
    execute [label="LLM\\nExecution", fillcolor="#FFEBEE"];
    evaluate [label="Reward\\nShell", fillcolor="#E8F5E8"];
    contradiction [label="Contradiction\\nAnalysis", fillcolor="#FFF3E0"];
    retry [label="Retry\\nAnalysis", fillcolor="#F1F8E9", shape=diamond];
    finalize [label="Finalize\\nResults", fillcolor="#E3F2FD"];
    end [label="END", shape=ellipse, fillcolor="#795548"];
    
    // Local LLM nodes
    mistral [label="Mistral\\nLocal", fillcolor="#FFCDD2"];
    llama [label="LLaMA\\nLocal", fillcolor="#FFCDD2"];
    deepseek [label="DeepSeek\\nLocal", fillcolor="#FFCDD2"];
    
    // Main flow
    start -> init;
    init -> health;
    health -> reasoning;
    reasoning -> selection;
    selection -> execute;
    execute -> evaluate;
    evaluate -> contradiction;
    contradiction -> retry;
    retry -> finalize;
    finalize -> end;
    
    // Model selection
    selection -> mistral [style=dashed, label="general"];
    selection -> llama [style=dashed, label="text"];
    selection -> deepseek [style=dashed, label="math"];
    mistral -> execute [style=dashed];
    llama -> execute [style=dashed];
    deepseek -> execute [style=dashed];
    
    // Conditional flows
    health -> end [label="unhealthy", color=red];
    evaluate -> finalize [label="score >= 0.8", color=green];
    contradiction -> reasoning [label="apply contradiction", color=orange];
    retry -> selection [label="retry", color=blue];
    retry -> finalize [label="max retries", color=red];
    
    // Subgraph for local models
    subgraph cluster_llms {
        label="Local LLM Models";
        style=filled;
        fillcolor="#F5F5F5";
        mistral; llama; deepseek;
    }
}"""
    
    def _generate_json_schema(self) -> Dict[str, Any]:
        """Genera schema JSON del sistema"""
        return {
            "system": {
                "name": "Agentius MCP",
                "version": "1.0.0",
                "description": "Autonomous Agent Runtime with Local LLMs and Contradiction Learning"
            },
            "architecture": {
                "type": "LangGraph StateGraph",
                "nodes": {
                    "initialize": {
                        "type": "memory",
                        "description": "Initialize session and configuration",
                        "inputs": ["user_input", "config"],
                        "outputs": ["session_id", "initial_state"]
                    },
                    "health_check": {
                        "type": "evaluation", 
                        "description": "Check system and model health",
                        "inputs": ["system_state"],
                        "outputs": ["health_status", "available_models"]
                    },
                    "reasoning": {
                        "type": "reasoning",
                        "description": "Enhanced reasoning with task analysis",
                        "inputs": ["user_input", "context"],
                        "outputs": ["task_plan", "complexity_score"]
                    },
                    "adaptive_selection": {
                        "type": "decision",
                        "description": "Auto-detect best model for task",
                        "inputs": ["task_plan", "available_models"],
                        "outputs": ["selected_model", "selection_reason"]
                    },
                    "execute_llm": {
                        "type": "execution",
                        "description": "Execute with local LLM and Langwatch tracking",
                        "inputs": ["prompt", "model_config"],
                        "outputs": ["response", "metrics", "tokens"]
                    },
                    "evaluate": {
                        "type": "evaluation",
                        "description": "Multi-dimensional quality assessment",
                        "inputs": ["response", "original_task"],
                        "outputs": ["score", "feedback", "quality_metrics"]
                    },
                    "contradiction_analysis": {
                        "type": "decision",
                        "description": "Detect failures and apply contradiction",
                        "inputs": ["score", "attempt_history"],
                        "outputs": ["contradiction_needed", "contradiction_prompt"]
                    },
                    "retry_analysis": {
                        "type": "decision", 
                        "description": "Intelligent retry strategy selection",
                        "inputs": ["score", "attempts", "contradiction_applied"],
                        "outputs": ["should_retry", "retry_strategy"]
                    },
                    "finalize": {
                        "type": "memory",
                        "description": "Save results and cleanup",
                        "inputs": ["final_response", "session_data"],
                        "outputs": ["saved_session", "cleanup_status"]
                    }
                },
                "edges": {
                    "conditional": [
                        {"from": "health_check", "to": "end", "condition": "system_unhealthy"},
                        {"from": "evaluate", "to": "finalize", "condition": "score >= 0.8"},
                        {"from": "contradiction_analysis", "to": "reasoning", "condition": "apply_contradiction"},
                        {"from": "retry_analysis", "to": "adaptive_selection", "condition": "should_retry"},
                        {"from": "retry_analysis", "to": "finalize", "condition": "max_retries"}
                    ],
                    "sequential": [
                        "start -> initialize",
                        "initialize -> health_check", 
                        "health_check -> reasoning",
                        "reasoning -> adaptive_selection",
                        "adaptive_selection -> execute_llm",
                        "execute_llm -> evaluate",
                        "evaluate -> contradiction_analysis",
                        "contradiction_analysis -> retry_analysis",
                        "retry_analysis -> finalize",
                        "finalize -> end"
                    ]
                }
            },
            "features": {
                "local_llms": {
                    "models": ["mistral-local", "llama-local", "deepseek-local"],
                    "format": ".gguf",
                    "auto_selection": True,
                    "fallback_enabled": True
                },
                "contradiction_learning": {
                    "enabled": True,
                    "intensities": ["mild", "moderate", "strong", "extreme"],
                    "effectiveness_tracking": True
                },
                "monitoring": {
                    "langwatch_integration": True,
                    "realtime_debugging": True,
                    "session_tracking": True,
                    "performance_metrics": True
                },
                "retry_strategies": {
                    "simple": "Same model, enhanced prompt",
                    "enhanced": "Different model, contradiction prompt", 
                    "alternative": "Alternative approach",
                    "decomposed": "Break into subtasks"
                }
            },
            "export_metadata": {
                "generated_at": datetime.now().isoformat(),
                "export_version": "1.0",
                "schema_version": "1.0"
            }
        }
    
    def export_session_report(self, session_id: str, session_data: Dict[str, Any]) -> str:
        """Exporta reporte completo de sesión"""
        report = {
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "summary": session_data.get("summary", {}),
            "performance": {
                "total_duration": session_data.get("total_duration", 0),
                "node_timings": session_data.get("node_timings", {}),
                "token_usage": session_data.get("token_usage", {}),
                "model_usage": session_data.get("model_usage", {})
            },
            "quality": {
                "final_score": session_data.get("final_score", 0),
                "score_progression": session_data.get("score_progression", []),
                "contradiction_applied": session_data.get("contradiction_applied", False),
                "contradiction_effectiveness": session_data.get("contradiction_effectiveness", 0)
            },
            "flow": {
                "nodes_visited": session_data.get("nodes_visited", []),
                "edges_taken": session_data.get("edges_taken", []),
                "retry_count": session_data.get("retry_count", 0),
                "error_count": session_data.get("error_count", 0)
            },
            "insights": self._generate_session_insights(session_data)
        }
        
        # Guardar reporte
        report_path = self.export_path / "reports" / f"session_{session_id}_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"Session report exported: {report_path}")
        return str(report_path)
    
    def _generate_session_insights(self, session_data: Dict[str, Any]) -> List[str]:
        """Genera insights automáticos de la sesión"""
        insights = []
        
        # Análisis de rendimiento
        total_duration = session_data.get("total_duration", 0)
        if total_duration > 30000:  # > 30 segundos
            insights.append("⚠️ Session took longer than expected - consider optimization")
        
        # Análisis de calidad
        final_score = session_data.get("final_score", 0)
        if final_score >= 0.9:
            insights.append("🎉 Excellent quality achieved on this session")
        elif final_score < 0.6:
            insights.append("📉 Low quality score - review contradiction strategy")
        
        # Análisis de contradicción
        if session_data.get("contradiction_applied", False):
            effectiveness = session_data.get("contradiction_effectiveness", 0)
            if effectiveness > 0.3:
                insights.append("🔥 Contradiction was highly effective")
            else:
                insights.append("🤔 Contradiction had limited impact - review strategy")
        
        # Análisis de reintentos
        retry_count = session_data.get("retry_count", 0)
        if retry_count > 2:
            insights.append("🔄 Multiple retries needed - task complexity may be high")
        
        return insights
    
    def create_studio_dashboard_config(self) -> str:
        """Crea configuración para dashboard de Studio"""
        config = {
            "dashboard": {
                "title": "Agentius MCP - LangGraph Studio",
                "refresh_interval": 5000,
                "auto_scroll": True
            },
            "panels": [
                {
                    "id": "system_overview",
                    "title": "System Overview",
                    "type": "metrics",
                    "metrics": [
                        "active_sessions",
                        "total_requests",
                        "avg_response_time",
                        "success_rate"
                    ]
                },
                {
                    "id": "model_performance", 
                    "title": "Model Performance",
                    "type": "chart",
                    "chart_type": "bar",
                    "data_source": "model_metrics"
                },
                {
                    "id": "contradiction_analysis",
                    "title": "Contradiction Effectiveness",
                    "type": "chart", 
                    "chart_type": "line",
                    "data_source": "contradiction_metrics"
                },
                {
                    "id": "session_flow",
                    "title": "Current Session Flow",
                    "type": "graph",
                    "graph_type": "mermaid",
                    "data_source": "current_session"
                }
            ],
            "alerts": [
                {
                    "condition": "error_rate > 0.1",
                    "message": "High error rate detected",
                    "severity": "warning"
                },
                {
                    "condition": "avg_response_time > 30000",
                    "message": "Response time degradation",
                    "severity": "warning"
                }
            ]
        }
        
        config_path = self.export_path / "studio_dashboard_config.json"
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        return str(config_path)

# Instancia global
export_manager = ExportManager()

def get_export_manager() -> ExportManager:
    """Obtiene instancia del export manager"""
    return export_manager


    
    def _generate_studio_readme(self) -> str:
        """Genera README para LangGraph Studio"""
        readme_content = f"""# LangGraph Studio - MCP System

## 🎯 Overview
This directory contains LangGraph Studio configuration and exported visualizations for the MCP (Model Context Protocol) system.

## 📊 Available Graphs

### 1. Complete MCP Agent (`mcp_complete_agent`)
- **Full orchestration flow** with all features
- **Reasoning + Reward + Contradiction + Retry**
- **Local LLMs integration** (Mistral, LLaMA, DeepSeek)
- **Langwatch monitoring** throughout

### 2. Reasoning Agent (`mcp_reasoning_agent`)
- **Specialized reasoning** with contradiction
- **Auto-model selection** based on task type
- **Enhanced prompting** with failure analysis

### 3. Builder Agent (`mcp_builder_agent`)
- **Content/code generation** specialist
- **Multiple output formats** (markdown, html, json, code)
- **Build type optimization** (website, code, document, analysis)

## 🔧 Studio Configuration

### Development Server
```bash
./langgraph_system/studio/studio.sh start
```

### Studio Access
- **Local**: http://localhost:8123
- **Tunnel**: Auto-generated public URL
- **Studio URL**: https://smith.langchain.com

### Debugging
- **Debug Port**: 8124
- **Hot Reload**: Enabled
- **Real-time Debugging**: Active

## 📈 Monitoring & Analytics

### Langwatch Integration
- **API Key**: Configured in .env
- **Session Tracking**: Enabled
- **Performance Metrics**: Active
- **Contradiction Analysis**: Automatic detection and effectiveness measurement

### Key Metrics Tracked
- **Token Usage**: Per model, per session
- **Response Time**: End-to-end latency
- **Quality Scores**: Multi-dimensional evaluation
- **Contradiction Effectiveness**: Improvement after contradiction
- **Retry Patterns**: Success rates and failure analysis

## 🎨 Exported Visualizations

### Mermaid Diagrams
- `mcp_complete_system.mmd` - Complete system flow
- `mcp_pitch_deck.mmd` - Simplified for presentations

### Usage in Documentation
```markdown
```mermaid
{{% include "studio_exports/graphs/mcp_complete_system.mmd" %}}
```
```

## 🚀 Key Features Visualized

### 1. **Intelligent Routing**
- Auto-detects best LLM for task type
- Fallback mechanisms for model failures
- Load balancing across local models

### 2. **Contradiction-Driven Improvement**
- Detects stagnation and low scores
- Injects explicit contradiction prompts
- Forces new approaches and methodologies

### 3. **Comprehensive Monitoring**
- Real-time performance tracking
- Session-based analytics
- Pattern recognition in failures

### 4. **Adaptive Retry Logic**
- Multiple retry strategies
- Escalation mechanisms
- Intelligent stopping conditions

## 🔍 Debugging Tips

### Studio Debugging
1. **Set breakpoints** in graph nodes
2. **Inspect state** at each step
3. **Monitor LLM calls** in real-time
4. **Analyze contradiction triggers**

### Performance Analysis
1. **Token efficiency** per model
2. **Response time** distribution
3. **Quality score** progression
4. **Retry success** patterns

## 📝 Configuration Files

- `langgraph.json` - Graph definitions and schemas
- `langgraph_studio_config.json` - Studio-specific settings
- `.env` - Environment variables and API keys

## 🎯 Next Steps

1. **Real-time Dashboard** - Live metrics visualization
2. **A/B Testing** - Compare contradiction strategies
3. **Pattern Learning** - ML-based failure prediction
4. **Cross-session Analytics** - User behavior analysis

---

Generated by LangGraph Studio Export Manager
Last updated: {self._get_timestamp()}
"""
        
        readme_path = self.export_path / "README.md"
        with open(readme_path, 'w') as f:
            f.write(readme_content)
        
        return str(readme_path)



# Instancia global
export_manager = ExportManager()

def get_export_manager() -> ExportManager:
    """Obtiene instancia del export manager"""
    return export_manager

