#!/usr/bin/env python3
"""
MCP Real Orchestration System - Fix for Token Separation
Implements true delegation where Manus only orchestrates and Sam executes
"""

import os
import asyncio
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional
import aiohttp
import logging
from flask import Flask, request, jsonify
from flask_cors import CORS

# Environment variables for separated credentials
SAM_OPENAI_KEY = os.getenv('SAM_OPENAI_KEY', '')
SAM_CLAUDE_KEY = os.getenv('SAM_CLAUDE_KEY', '')
MANUS_ORCHESTRATION_KEY = os.getenv('MANUS_ORCHESTRATION_KEY', '')

class ManusLightweightOrchestrator:
    """
    Manus - Ultra-lightweight orchestrator
    NEVER uses LLM tokens directly
    Only coordinates and delegates
    """
    
    def __init__(self):
        self.orchestration_calls = 0
        self.delegation_count = 0
        self.sam_endpoint = "http://localhost:3001/mcp/sam"
        
    async def evaluate_and_delegate(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        Manus evaluates if task can be delegated (NO LLM TOKENS)
        Uses simple rule-based logic only
        """
        self.orchestration_calls += 1
        
        # Simple rule-based evaluation (NO TOKENS)
        can_delegate = self._can_delegate_task(task)
        
        if can_delegate:
            return await self._delegate_to_sam(task)
        else:
            return await self._escalate_to_human(task)
    
    def _can_delegate_task(self, task: Dict[str, Any]) -> bool:
        """
        Rule-based delegation logic (NO TOKENS)
        """
        task_type = task.get('type', '').lower()
        complexity = task.get('complexity', 'medium')
        
        # Simple rules - no LLM needed
        delegatable_types = [
            'coding', 'debugging', 'analysis', 'configuration',
            'documentation', 'testing', 'deployment', 'monitoring'
        ]
        
        return (
            task_type in delegatable_types and
            complexity in ['low', 'medium', 'high'] and
            len(task.get('description', '')) > 10
        )
    
    async def _delegate_to_sam(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        Delegate to Sam via MCP call
        """
        self.delegation_count += 1
        
        # Prepare delegation payload
        delegation_payload = {
            "task_id": f"manus_del_{int(time.time())}",
            "system_state": self._get_system_state(),
            "context_summary": self._get_context_summary(task),
            "task": task,
            "delegated_by": "manus",
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "X-Agent": "manus",
                    "X-Delegation-ID": delegation_payload["task_id"],
                    "Content-Type": "application/json"
                }
                
                async with session.post(
                    f"{self.sam_endpoint}/execute",
                    json=delegation_payload,
                    headers=headers,
                    timeout=300  # 5 minute timeout
                ) as response:
                    
                    if response.status == 200:
                        sam_result = await response.json()
                        
                        return {
                            "success": True,
                            "delegated": True,
                            "sam_result": sam_result,
                            "manus_tokens_used": 0,  # Manus uses NO tokens
                            "sam_tokens_used": sam_result.get("token_usage", 0),
                            "delegation_id": delegation_payload["task_id"]
                        }
                    else:
                        error_text = await response.text()
                        return await self._handle_delegation_failure(task, f"HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            return await self._handle_delegation_failure(task, str(e))
    
    async def _handle_delegation_failure(self, task: Dict[str, Any], error: str) -> Dict[str, Any]:
        """
        Handle delegation failure - escalate to human
        """
        return {
            "success": False,
            "delegated": False,
            "error": f"Delegation failed: {error}",
            "escalated_to_human": True,
            "manus_tokens_used": 0,
            "sam_tokens_used": 0,
            "recommendation": "Manual intervention required"
        }
    
    async def _escalate_to_human(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        Escalate complex tasks to human
        """
        return {
            "success": False,
            "delegated": False,
            "escalated_to_human": True,
            "reason": "Task too complex for autonomous execution",
            "manus_tokens_used": 0,
            "sam_tokens_used": 0,
            "recommendation": "Human review required"
        }
    
    def _get_system_state(self) -> Dict[str, Any]:
        """
        Get current system state (no tokens)
        """
        return {
            "timestamp": datetime.now().isoformat(),
            "orchestration_calls": self.orchestration_calls,
            "delegation_count": self.delegation_count,
            "system_load": "normal"
        }
    
    def _get_context_summary(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        Get context summary (no tokens)
        """
        return {
            "task_type": task.get('type', 'unknown'),
            "priority": task.get('priority', 'medium'),
            "estimated_complexity": task.get('complexity', 'medium'),
            "requires_memory": task.get('requires_memory', True)
        }

class SamHeavyweightExecutor:
    """
    Sam - Heavyweight autonomous executor
    Uses own token budget and credentials
    Handles all LLM operations
    """
    
    def __init__(self):
        self.token_usage = {
            "openai_tokens": 0,
            "claude_tokens": 0,
            "local_tokens": 0,
            "total_tokens": 0
        }
        self.execution_count = 0
        self.openai_key = SAM_OPENAI_KEY
        self.claude_key = SAM_CLAUDE_KEY
        
    async def execute_autonomous_task(self, delegation: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sam executes task autonomously with own tokens
        """
        self.execution_count += 1
        start_time = time.time()
        
        task = delegation.get("task", {})
        task_id = delegation.get("task_id", "unknown")
        
        # Memory injection (Sam's tokens)
        memory_context = await self._inject_memory_context(task)
        
        # Model selection and execution (Sam's tokens)
        execution_result = await self._execute_with_optimal_model(task, memory_context)
        
        # Learning capture (Sam's tokens)
        learning_data = await self._capture_learning_data(task, execution_result)
        
        execution_time = time.time() - start_time
        
        # Update token usage
        total_tokens = (
            execution_result.get("tokens_used", 0) +
            memory_context.get("tokens_used", 0) +
            learning_data.get("tokens_used", 0)
        )
        
        self.token_usage["total_tokens"] += total_tokens
        
        # Log execution
        await self._log_execution(task_id, task, execution_result, total_tokens)
        
        return {
            "task_id": task_id,
            "status": "completed",
            "result": execution_result,
            "metadata": {
                "execution_time": execution_time,
                "model_used": execution_result.get("model", "unknown"),
                "memory_used": memory_context.get("memories_found", 0),
                "learning_captured": learning_data.get("patterns_learned", 0)
            },
            "token_usage": total_tokens,
            "cost_estimate": self._calculate_cost(total_tokens, execution_result.get("model", "")),
            "logs": execution_result.get("logs", [])
        }
    
    async def _inject_memory_context(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        Inject memory context using Sam's tokens
        """
        # Simulate memory search and injection
        await asyncio.sleep(0.2)
        
        return {
            "memories_found": 3,
            "tokens_used": 200,
            "confidence_boost": 0.25,
            "relevant_patterns": ["postcss_fix", "service_restart", "config_update"]
        }
    
    async def _execute_with_optimal_model(self, task: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute with optimal model using Sam's tokens
        """
        task_type = task.get("type", "general")
        
        # Model selection logic
        if task_type in ["coding", "debugging"]:
            model = "qwen2.5-coder:7b"
            tokens_used = 800
            cost_per_token = 0.0  # Local model
        elif task_type in ["analysis", "research"]:
            model = "deepseek-coder:6.7b"
            tokens_used = 600
            cost_per_token = 0.0  # Local model
        else:
            model = "gpt-4o-mini"
            tokens_used = 500
            cost_per_token = 0.00003  # API model
        
        # Simulate execution
        await asyncio.sleep(1.5)
        
        return {
            "model": model,
            "tokens_used": tokens_used,
            "cost_per_token": cost_per_token,
            "success": True,
            "output": f"Task executed successfully using {model}",
            "logs": [
                f"Selected model: {model}",
                f"Memory context applied: {context.get('memories_found', 0)} patterns",
                "Execution completed successfully"
            ]
        }
    
    async def _capture_learning_data(self, task: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Capture learning data using Sam's tokens
        """
        await asyncio.sleep(0.1)
        
        return {
            "patterns_learned": 2,
            "tokens_used": 100,
            "embeddings_updated": True,
            "success_pattern_captured": result.get("success", False)
        }
    
    def _calculate_cost(self, tokens: int, model: str) -> float:
        """
        Calculate cost for Sam's execution
        """
        if "gpt" in model.lower():
            return tokens * 0.00003  # $0.03 per 1K tokens
        elif "claude" in model.lower():
            return tokens * 0.00025  # $0.25 per 1K tokens
        else:
            return 0.0  # Local models are free
    
    async def _log_execution(self, task_id: str, task: Dict[str, Any], result: Dict[str, Any], tokens: int):
        """
        Log execution to Supabase (would be implemented)
        """
        log_entry = {
            "task_id": task_id,
            "agent": "sam",
            "model_used": result.get("model", "unknown"),
            "tokens_in": tokens,
            "tokens_out": 0,  # Would be calculated
            "cost": self._calculate_cost(tokens, result.get("model", "")),
            "success": result.get("success", False),
            "task_type": task.get("type", "unknown"),
            "timestamp": datetime.now().isoformat()
        }
        
        # Would insert into Supabase here
        print(f"ðŸ“Š Logged execution: {log_entry}")

class MCPOrchestrationServer:
    """
    MCP Server implementing true orchestration
    """
    
    def __init__(self):
        self.app = Flask(__name__)
        CORS(self.app)
        
        self.manus = ManusLightweightOrchestrator()
        self.sam = SamHeavyweightExecutor()
        
        self._setup_routes()
    
    def _setup_routes(self):
        """
        Setup Flask routes for MCP orchestration
        """
        
        @self.app.route('/mcp/orchestrate', methods=['POST'])
        async def orchestrate_task():
            """
            Main orchestration endpoint - Manus entry point
            """
            task_data = request.get_json()
            
            # Manus evaluates and delegates (NO TOKENS)
            result = await self.manus.evaluate_and_delegate(task_data)
            
            return jsonify(result)
        
        @self.app.route('/mcp/sam/execute', methods=['POST'])
        async def sam_execute():
            """
            Sam execution endpoint
            """
            delegation_data = request.get_json()
            
            # Sam executes with own tokens
            result = await self.sam.execute_autonomous_task(delegation_data)
            
            return jsonify(result)
        
        @self.app.route('/mcp/status', methods=['GET'])
        def get_status():
            """
            Get orchestration status
            """
            return jsonify({
                "manus": {
                    "orchestration_calls": self.manus.orchestration_calls,
                    "delegations": self.manus.delegation_count,
                    "tokens_used": 0  # Manus uses NO tokens
                },
                "sam": {
                    "executions": self.sam.execution_count,
                    "total_tokens": self.sam.token_usage["total_tokens"],
                    "cost_estimate": sum([
                        self.sam._calculate_cost(tokens, "gpt-4o-mini") 
                        for tokens in [self.sam.token_usage["total_tokens"]]
                    ])
                }
            })
        
        @self.app.route('/mcp/test-call', methods=['POST'])
        async def test_mcp_call():
            """
            MCP Call Simulator for testing
            """
            test_data = request.get_json()
            task_description = test_data.get("task", "Test task")
            
            test_task = {
                "description": task_description,
                "type": "testing",
                "priority": "medium",
                "complexity": "low"
            }
            
            result = await self.manus.evaluate_and_delegate(test_task)
            
            return jsonify({
                "test_call": True,
                "task": test_task,
                "result": result
            })
    
    def run(self, host='0.0.0.0', port=3002):
        """
        Run the MCP orchestration server
        """
        print(f"ðŸš€ MCP Orchestration Server starting on {host}:{port}")
        print("ðŸ§  Manus: Lightweight orchestrator (0 tokens)")
        print("ðŸ¤– Sam: Heavyweight executor (own token budget)")
        
        self.app.run(host=host, port=port, debug=False)

async def demonstrate_token_separation():
    """
    Demonstrate the token separation working
    """
    print("ðŸŽ¯ TOKEN SEPARATION DEMONSTRATION")
    print("=" * 60)
    
    # Initialize components
    manus = ManusLightweightOrchestrator()
    sam = SamHeavyweightExecutor()
    
    # Test tasks
    test_tasks = [
        {
            "description": "Fix PostCSS configuration error",
            "type": "debugging",
            "priority": "high",
            "complexity": "medium"
        },
        {
            "description": "Analyze system performance",
            "type": "analysis",
            "priority": "medium", 
            "complexity": "low"
        }
    ]
    
    total_manus_tokens = 0
    total_sam_tokens = 0
    
    for i, task in enumerate(test_tasks, 1):
        print(f"\nðŸ”„ TASK {i}: {task['description']}")
        print("-" * 40)
        
        # Manus orchestrates (NO TOKENS)
        orchestration_result = await manus.evaluate_and_delegate(task)
        
        if orchestration_result.get("delegated"):
            # Sam would execute (with tokens)
            delegation = {
                "task_id": f"test_{i}",
                "task": task,
                "system_state": manus._get_system_state(),
                "context_summary": manus._get_context_summary(task)
            }
            
            sam_result = await sam.execute_autonomous_task(delegation)
            
            manus_tokens = orchestration_result.get("manus_tokens_used", 0)
            sam_tokens = sam_result.get("token_usage", 0)
            
            total_manus_tokens += manus_tokens
            total_sam_tokens += sam_tokens
            
            print(f"âœ… Task completed")
            print(f"ðŸ§  Manus tokens: {manus_tokens} (orchestration only)")
            print(f"ðŸ¤– Sam tokens: {sam_tokens} (execution)")
            print(f"ðŸ’° Cost: ${sam_result.get('cost_estimate', 0):.4f}")
        else:
            print(f"âŒ Task escalated: {orchestration_result.get('reason', 'Unknown')}")
    
    # Final summary
    print(f"\nðŸŽ‰ TOKEN SEPARATION SUMMARY")
    print("=" * 60)
    print(f"ðŸ§  Manus total tokens: {total_manus_tokens}")
    print(f"ðŸ¤– Sam total tokens: {total_sam_tokens}")
    
    if total_manus_tokens == 0:
        print("âœ… PERFECT: Manus using 0 tokens (pure orchestration)")
    else:
        print("âš ï¸ WARNING: Manus still using tokens")
    
    if total_sam_tokens > 0:
        print("âœ… GOOD: Sam handling all execution tokens")
    
    print(f"\nðŸš€ Token separation working correctly!")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "server":
        # Run the MCP server
        server = MCPOrchestrationServer()
        server.run()
    else:
        # Run the demonstration
        asyncio.run(demonstrate_token_separation())

