version: '3.8'

services:
  # Local Models Orchestrator
  local-models-orchestrator:
    build: ./services/local-models-orchestrator
    container_name: ultramcp-local-models-orchestrator
    ports:
      - "8012:8012"
    environment:
      - PYTHONUNBUFFERED=1
    volumes:
      - ./logs:/app/logs
    networks:
      - ultramcp-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8012/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - ollama-proxy

  # Ollama Proxy (connects to host Ollama)
  ollama-proxy:
    image: nginx:alpine
    container_name: ultramcp-ollama-proxy
    ports:
      - "11435:80"  # Proxy for host Ollama on 11434
    volumes:
      - ./config/nginx/ollama-proxy.conf:/etc/nginx/conf.d/default.conf
    networks:
      - ultramcp-network
    restart: unless-stopped

  # Kimi-K2 Local Model (when needed)
  kimi-k2-local:
    build: ./services/kimi-k2-local
    container_name: ultramcp-kimi-k2
    ports:
      - "8011:8011"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_NAME=moonshot-ai/Kimi-K2-Instruct
    volumes:
      - kimi_model_cache:/root/.cache/huggingface
      - ./logs:/app/logs
    networks:
      - ultramcp-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    profiles:
      - kimi  # Only start when explicitly requested

networks:
  ultramcp-network:
    external: true

volumes:
  kimi_model_cache:
    name: ultramcp-kimi-model-cache