# üåü Enhanced CoD Protocol Setup Guide

*World's First Hybrid Local+API Multi-LLM Debate System*

## Revolutionary Local LLM + Chain-of-Debate Integration

### üöÄ Quick Start

1. **System Setup:**
   ```bash
   # Clone and setup UltraMCP
   git clone https://github.com/fmfg03/ultramcp.git
   cd ultramcp
   make setup
   ```

2. **Verify Local Models (5 models, 17+ GB):**
   ```bash
   make local-models
   # Should show: qwen2.5:14b, llama3.1:8b, qwen2.5-coder:7b, mistral:7b, deepseek-coder:6.7b
   ```

3. **Test Revolutionary System:**
   ```bash
   # 100% local debate (zero cost, maximum privacy)
   make cod-local TOPIC="Should we use microservices or monolith architecture?"
   ```

4. **Explore All Debate Modes:**
   ```bash
   make cod-hybrid TOPIC="AI ethics in autonomous vehicles"        # Best of both worlds
   make cod-privacy TOPIC="Internal security policy changes"       # Privacy-first
   make cod-cost-optimized TOPIC="Cloud migration strategy"        # Minimize costs
   make dev-decision DECISION="React vs Vue for this component?"   # Quick dev choices
   ```

### Available Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `make cod-local` | Local-only debate | Privacy-sensitive topics |
| `make cod-hybrid` | Mixed local+API | Best quality results |
| `make cod-privacy` | Privacy-first | Confidential discussions |
| `make cod-cost-optimized` | Cost-efficient | Budget-conscious analysis |
| `make dev-decision` | Quick local decision | Development choices |
| `make claude-debate` | Claude Code optimized | Development workflows |
| `make test-cod-performance` | Performance testing | System optimization |

### üéØ Revolutionary Integration Benefits

#### üîí Privacy & Security
- **100% Privacy Option**: Sensitive topics never leave your machine
- **Offline Capability**: Works completely without internet
- **Enterprise-Grade**: No data sent to external APIs in local mode
- **Privacy Scoring**: Real-time privacy metrics for each debate

#### üí∞ Cost & Performance
- **Zero API Costs**: Local models are free after initial download
- **Unlimited Usage**: No rate limits or quotas on local models
- **17+ GB of AI Power**: 5 specialized models for different perspectives
- **Smart Model Selection**: Automatic selection based on task requirements

#### üß† Intelligence & Roles
- **Multi-Perspective Analysis**: CFO, CTO, Analyst, Visionary, Devil's Advocate roles
- **Intelligent Consensus**: Advanced consensus algorithms with confidence scoring
- **Hybrid Coordination**: Seamless integration of local and API models
- **Specialized Models**: Coding experts, analytical thinkers, strategic visionaries

#### ‚ö° Developer Experience
- **Claude Code Optimized**: Purpose-built for developer productivity
- **Terminal-First Design**: 80% terminal commands, 20% advanced orchestration
- **Instant Feedback**: Real-time debate results and model performance metrics
- **Development Integration**: Quick decisions for code architecture and design

### üõ†Ô∏è Advanced Setup & Configuration

#### System Requirements
- **RAM**: 16GB+ recommended (8GB minimum)
- **Storage**: 20GB+ for all 5 models
- **CPU**: Multi-core recommended for parallel model execution
- **OS**: Linux, macOS, Windows (with WSL2)

#### Model Specifications
| Model | Size | Specialization | Use Case |
|-------|------|----------------|----------|
| Qwen 2.5 14B | 9.0 GB | Strategic analysis, complex reasoning | Enterprise decisions |
| Llama 3.1 8B | 4.9 GB | Balanced analysis, general reasoning | Versatile discussions |
| Qwen Coder 7B | 4.7 GB | Technical analysis, software architecture | Development decisions |
| Mistral 7B | 4.1 GB | Quick analysis, practical perspectives | Rapid iterations |
| DeepSeek Coder 6.7B | 3.8 GB | Advanced technical evaluation | System design |

### üîß Troubleshooting

#### Models Not Responding
```bash
# Check local model status
make local-status
make local-models

# Verify Ollama service
ollama ps
ollama serve  # If not running
```

#### Performance Optimization
```bash
# Test model performance
make test-cod-performance

# Check system resources
htop  # Monitor CPU/RAM usage

# Optimize model selection
make cod-local TOPIC="test" --rounds=1  # Quick test
```

#### Installation Issues
```bash
# Install missing dependencies
pip3 install aiohttp asyncio

# Verify Python environment
python3 --version  # Should be 3.8+

# Check Docker services
make docker-logs
make status
```

#### Model Download Issues
```bash
# Download individual models
make local-pull MODEL="qwen2.5:14b"
make local-pull MODEL="llama3.1:8b"

# Check disk space
df -h

# Remove unused models
make local-remove MODEL="unused-model"
```
